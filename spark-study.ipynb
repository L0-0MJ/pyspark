{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8401766c-f1d1-4659-84ad-d50137bd1530",
   "metadata": {},
   "source": [
    "### empty RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb82cf62-cb76-4f16-96c3-0a9f0e7abdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "print(emptyRDD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60bb699-df1f-4df7-8881-81c134735cf1",
   "metadata": {},
   "source": [
    "### parallelize로 RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfb33be-fb7a-4f5a-b4b3-fb3f90a918f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287\n"
     ]
    }
   ],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([])\n",
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c879e8d-b74c-4cbb-ab30-d6e381202f48",
   "metadata": {},
   "source": [
    "### 스키마(Struct type)을 사용해서 empty DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ed0892-b231-4a43-bfd8-7f21be9a4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('firstname' , StringType(), True),\n",
    "    StructField('middle', StringType(), True),\n",
    "    StructField('lastname', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b807c36-52c1-4268-aa57-5082b405fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middle: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(emptyRDD, schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb03d29-edae-4134-b931-d5e266710536",
   "metadata": {},
   "source": [
    "### empty rdd-> Dataframe 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6d41bb-9364-4c74-a95e-49415d3ca160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middle: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbc1b7-7f17-498b-aa77-596fd33606cb",
   "metadata": {},
   "source": [
    "### 스키마 없이 빈 DataFrame 생성(열없음)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2bb43c5-24b0-484e-8a5d-b5f195056da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40c1ad-2805-49b4-8531-2cdeac4f3a82",
   "metadata": {},
   "source": [
    "### RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f210de-78f8-40bd-aa5f-b6758c1ff871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a17859b8-ce95-4dd0-b5b4-9a8c04f57571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:287"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7bdee2-df25-46db-a200-5da6525af1fb",
   "metadata": {},
   "source": [
    "### RDD -> df 변환 (toDF() 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e7a150-8af4-4c21-8a0a-eb24e07742af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate = False) #생략없이 다 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f933da3-e9f7-404c-b9d2-995507015fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc2a36-c050-4542-ae9c-7a36033a48c4",
   "metadata": {},
   "source": [
    "### createDataFrame() 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0420c172-06dd-4621-bbd8-5501fb2e303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69a491-4cf9-4df8-9051-17da90342afc",
   "metadata": {},
   "source": [
    "### StructType 스키마 createDataFrame()사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7542161a-51ae-4c6c-9389-b68d08569b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "deptSchema = StructType([\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c192a8-370a-4edc-a8ac-8631ebf03ca3",
   "metadata": {},
   "source": [
    "### DataFrame -> Pandas\n",
    "PySpark에서 데이터를 처리한 후 Machine Learning 애플리케이션 또는 Python 애플리케이션을 사용한 추가 처리를 위해 데이터를 Pandas DataFrame으로 다시 변환해야 합니다.\n",
    "Pandas는 단일 노드에서 작업을 실행하는 반면 PySpark는 여러 시스템에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c73cc614-c69d-41d0-9dce-c7eb75129111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|dob  |gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|James     |           |Smith    |36636|M     |60000 |\n",
      "|Michael   |Rose       |         |40288|M     |70000 |\n",
      "|Robert    |           |Williams |42114|      |400000|\n",
      "|Maria     |Anne       |Jones    |39192|F     |500000|\n",
      "|Jen       |Mary       |Brown    |     |F     |0     |\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\", \"middle_name\", \"last_name\", \"dob\", \"gender\", \"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5c365-4f5b-4f28-89ee-3bec81f815bd",
   "metadata": {},
   "source": [
    "pandas add a sequence number to the result as a row Index. You can rename pandas columns by using rename() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23677315-269a-4e29-a6a6-3a479a525fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3      Maria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "pandasDF = pysparkDF.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb454c5d-145b-4a9d-b81e-abe3c8442e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c4752b3-7fc7-45fb-ae89-ccca6a7eca6b",
   "metadata": {},
   "source": [
    "### show() \n",
    "By default, it shows only 20 Rows, and the column values are truncated at 20 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8485c85-9a4f-4ba8-ac3b-c1e7bf58a88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Finance| 10|\n",
      "|Marketing| 20|\n",
      "|    Sales| 30|\n",
      "|       IT| 40|\n",
      "+---------+---+\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "+---------+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Finance| 10|\n",
      "|Marketing| 20|\n",
      "+---------+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0--------\n",
      " _1  | Finance   \n",
      " _2  | 10        \n",
      "-RECORD 1--------\n",
      " _1  | Marketing \n",
      " _2  | 20        \n",
      "-RECORD 2--------\n",
      " _1  | Sales     \n",
      " _2  | 30        \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "\n",
    "df.show(truncate = False) # full column contents\n",
    "\n",
    "df.show(2, truncate = False) # 2 row , full column contents\n",
    "\n",
    "df.show(2, truncate = 25) # 2 row, column values 25 characters\n",
    "\n",
    "df.show(n=3, truncate= 25, vertical= True) # DataFrame rows & columns vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de904cf-a81b-45cd-bfdb-b57e31993bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|Seqno|               Quote|\n",
      "+-----+--------------------+\n",
      "|    1|Be the change tha...|\n",
      "|    2|Everyone thinks o...|\n",
      "|    3|The purpose of ou...|\n",
      "|    4|            Be cool.|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "columns = [\"Seqno\", \"Quote\"]\n",
    "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
    "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
    "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
    "    (\"4\", \"Be cool.\")]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40737bd2-79d7-428e-84d7-29db523ca9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+\n",
      "|Seqno|Quote                                                                        |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "|1    |Be the change that you wish to see in the world                              |\n",
      "|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n",
      "|3    |The purpose of our lives is to be happy.                                     |\n",
      "|4    |Be cool.                                                                     |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf1c50f-74ad-4e41-96ef-866bc5cdfbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+\n",
      "|Seqno|Quote                                                                        |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "|1    |Be the change that you wish to see in the world                              |\n",
      "|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b063907c-44e2-4660-afa7-b52b681a9904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+\n",
      "|Seqno|                    Quote|\n",
      "+-----+-------------------------+\n",
      "|    1|Be the change that you...|\n",
      "|    2|Everyone thinks of cha...|\n",
      "+-----+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, truncate=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e178e6d-eeff-448e-9292-aaa0ccc3010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " Seqno | 1                         \n",
      " Quote | Be the change that you... \n",
      "-RECORD 1--------------------------\n",
      " Seqno | 2                         \n",
      " Quote | Everyone thinks of cha... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, truncate=25, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59355e-5368-4481-aa70-c874cc666699",
   "metadata": {},
   "source": [
    "### StructType, StructField \n",
    "programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns\n",
    "프로그래밍 방식으로 데이터프레임에 대한 스키마를 지정하고, 중첩된 구조체, 배열, map 컬럼 같은 복잡한 컬럼을 만든는데 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bacc18-4b22-4da9-890c-7484f16e5cf6",
   "metadata": {},
   "source": [
    "StructType 데이터프레임 구조 정의\n",
    "pyspark.sql.types import StructType\n",
    "\n",
    "Structfield 데이터프레임 컬럼의 메타데이터 정의\n",
    "pyspark.sql.types import StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba8418-52a7-43f3-b3fd-5ee54e6ee511",
   "metadata": {},
   "source": [
    "### Nested StructType object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3b43425-b2af-4ffa-80fd-7a4fbe7d34b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa579ed5-0387-4f4c-97a2-ebbc32029d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "structureData = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d2578f-24d9-4dbf-a56f-b3b0ceffb0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame(data = structureData, schema = structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df02915-6fb7-4802-91d8-431bc284b37e",
   "metadata": {},
   "source": [
    "### DataFrame의 Struct 추가, 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6092917c-3588-4ab4-8c52-2aa379de2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,struct,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f5cea51-a4fc-401a-8998-d4a6b743d47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- OtherInfo: struct (nullable = false)\n",
      " |    |-- identifier: string (nullable = true)\n",
      " |    |-- gender: string (nullable = true)\n",
      " |    |-- salary: integer (nullable = true)\n",
      " |    |-- Salary_Grade: string (nullable = false)\n",
      "\n",
      "+--------------------+------------------------+\n",
      "|name                |OtherInfo               |\n",
      "+--------------------+------------------------+\n",
      "|{James, , Smith}    |{36636, M, 3100, Medium}|\n",
      "|{Michael, Rose, }   |{40288, M, 4300, High}  |\n",
      "|{Robert, , Williams}|{42114, M, 1400, Low}   |\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n",
      "|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedDF = df2.withColumn(\"OtherInfo\", struct(col(\"id\").alias(\"identifier\"),\n",
    "                                        col(\"gender\").alias(\"gender\"),\n",
    "                                               col(\"salary\").alias(\"salary\"),\n",
    "                                               when(col(\"salary\").cast(IntegerType()) < 2000, \"Low\")\n",
    "                                               .when(col(\"salary\").cast(IntegerType()) < 4000, \"Medium\")\n",
    "                                               .otherwise(\"High\").alias(\"Salary_Grade\")\n",
    "                                              )).drop(\"id\", \"gender\", \"salary\")\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2bf069-7ed7-4a50-8abf-f6ae2743f845",
   "metadata": {},
   "source": [
    "### SQL ArrayType & MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20692a78-a8e3-41ae-ab85-bd17931e8dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "arrayStructureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)])),\n",
    "    StructField('hobbies', ArrayType(StringType()), True),\n",
    "    StructField('properties', MapType(StringType(), StringType()), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02bf5aa1-604b-4670-8c46-33d878f68dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('name', StructType([StructField('firstname', StringType(), True), StructField('middlename', StringType(), True), StructField('lastname', StringType(), True)]), True), StructField('hobbies', ArrayType(StringType(), True), True), StructField('properties', MapType(StringType(), StringType(), True), True)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayStructureSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "986ec009-9c83-45dc-bfa4-213d273d4e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\":[{\"metadata\":{},\"name\":\"name\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"firstname\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"middlename\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"lastname\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"gender\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"salary\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}\n"
     ]
    }
   ],
   "source": [
    "print(df2.schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14361fb-a2e7-470b-b18e-837fe2713719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "schemaFromJson = StructType.fromJson(json.loads(schema.json))\n",
    "df3 = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(structureData), schemaFromJson)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a8e99-6957-470e-96bc-c5f7e2e05d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb7ddc72-af63-4bfa-bf99-1c8bdafdcac8",
   "metadata": {},
   "source": [
    "### column class object 생성\n",
    "pyspark.sql.Column\\\n",
    "manipulate the Column values, evaluate the boolean expression to filter rows, retrieve a value or part of a value from a DataFrame column, and to work with list, map & struct columns.\\\n",
    "컬럼 값을 조작하거나, 부울 표현식으로 평가해서 로우를 필터링, 데이터프레임 컬럼에서 일부 또는 전체를 검색, 리스트, 맵, 구조체 컬럼을 사용함?..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fff86bbd-638f-4adc-a166-152b2f73322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column class object using lit()\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "colObj = lit(\"sparkbyexample.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2171000-ccfa-41c8-aeb4-1ce50a20c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\",23), (\"Ann\", 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da9e670a-37d9-4f87-95ab-5383b181e136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name.fname: string (nullable = true)\n",
      " |-- gender: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data).toDF(\"name.fname\", \"gender\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "665d02b0-cf07-4a6e-ae09-d110a101d408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|       gender|\n",
      "+-------------+\n",
      "|{black, blue}|\n",
      "|{grey, black}|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|       gender|\n",
      "+-------------+\n",
      "|{black, blue}|\n",
      "|{grey, black}|\n",
      "+-------------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "+----------+\n",
      "\n",
      "+-------------+\n",
      "|       gender|\n",
      "+-------------+\n",
      "|{black, blue}|\n",
      "|{grey, black}|\n",
      "+-------------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.gender).show()\n",
    "df.select(df[\"gender\"]).show()\n",
    "df.select(df[\"`name.fname`\"]).show()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"gender\")).show()    \n",
    "df.select(col(\"`name.fname`\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bffc060-b551-4169-b3e2-ab02374c07bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "data = [Row(name = \"James\", prop= Row(hair = \"black\", eye = \"blue\")),\n",
    "Row(name = \"Ann\", prop = Row(hair = \"grey\", eye = \"black\"))]\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27037772-69ec-4324-b34a-d8017512b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|prop.hair|\n",
      "+---------+\n",
      "|    black|\n",
      "|     grey|\n",
      "+---------+\n",
      "\n",
      "+-----+\n",
      "| hair|\n",
      "+-----+\n",
      "|black|\n",
      "| grey|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| hair|\n",
      "+-----+\n",
      "|black|\n",
      "| grey|\n",
      "+-----+\n",
      "\n",
      "+-----+-----+\n",
      "| hair|  eye|\n",
      "+-----+-----+\n",
      "|black| blue|\n",
      "| grey|black|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.prop.hair).show()\n",
    "df.select(df[\"prop.hair\"]).show()\n",
    "df.select(col(\"prop.hair\")).show()\n",
    "\n",
    "df.select(col(\"prop.*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b26d0e-fd3b-4e0a-8967-b5f6e97080d8",
   "metadata": {},
   "source": [
    "### Select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78bd5122-294b-4967-bee5-2adae77d1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "       (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")]\n",
    "columns = [\"firstname\", \"lastname\", \"country\", \"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95e66d0b-12d2-4bfa-aea0-8a203f3b95c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"firstname\", \"lastname\").show()\n",
    "df.select(df.firstname, df.lastname).show()\n",
    "df.select(df[\"firstname\"], df[\"lastname\"]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "faea3ed4-5c1c-4d67-a9fe-0df2295cef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#col()\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"firstname\"), col(\"lastname\")).show()\n",
    "#정규식\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "50c45cef-e669-4915-b00e-b43e17903981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(*columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "76490b56-b1f8-458a-ae02-994dc47829f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모든 열 선택\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3713814d-7230-4c06-9b29-bb5fb5fcd7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+\n",
      "|firstname|lastname|country|\n",
      "+---------+--------+-------+\n",
      "|    James|   Smith|    USA|\n",
      "|  Michael|    Rose|    USA|\n",
      "|   Robert|Williams|    USA|\n",
      "+---------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-----+\n",
      "|country|state|\n",
      "+-------+-----+\n",
      "|    USA|   CA|\n",
      "|    USA|   NY|\n",
      "|    USA|   CA|\n",
      "+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#인덱스 별로 선택\n",
    "df.select(df.columns[:3]).show(3)\n",
    "df.select(df.columns[2:4]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a0a88ae-1196-4578-8313-f47f3aa4d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "96358da7-3fe4-46b8-854c-605249b546ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+-----+------+\n",
      "|name                  |state|gender|\n",
      "+----------------------+-----+------+\n",
      "|{James, null, Smith}  |OH   |M     |\n",
      "|{Anna, Rose, }        |NY   |F     |\n",
      "|{Julia, , Williams}   |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |NY   |M     |\n",
      "|{Mike, Mary, Williams}|OH   |M     |\n",
      "+----------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "df2 = spark.createDataFrame(data = data, schema = schema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d60ad6a-4913-463c-bb85-9846be1652cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|name                  |\n",
      "+----------------------+\n",
      "|{James, null, Smith}  |\n",
      "|{Anna, Rose, }        |\n",
      "|{Julia, , Williams}   |\n",
      "|{Maria, Anne, Jones}  |\n",
      "|{Jen, Mary, Brown}    |\n",
      "|{Mike, Mary, Williams}|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e507b14a-26bc-49ce-8759-c652fabe86e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|James    |Smith   |\n",
      "|Anna     |        |\n",
      "|Julia    |Williams|\n",
      "|Maria    |Jones   |\n",
      "|Jen      |Brown   |\n",
      "|Mike     |Williams|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name.firstname\", \"name.lastname\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8c5ea97-f626-4ee4-9bab-2a997c07bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "|James    |null      |Smith   |\n",
      "|Anna     |Rose      |        |\n",
      "|Julia    |          |Williams|\n",
      "|Maria    |Anne      |Jones   |\n",
      "|Jen      |Mary      |Brown   |\n",
      "|Mike     |Mary      |Williams|\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name.*\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d540ebd-d5d9-403a-b06f-ada02ea9ff6d",
   "metadata": {},
   "source": [
    "### collect()\n",
    "데이터 세트의 모든 요소를 드라이버 노드로 검색하는 데 사용되는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45221211-7cad-41c0-b957-6ebec68b263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dept = [(\"Finance\", 10),\\\n",
    "       (\"Marketing\", 20),\\\n",
    "       (\"Sales\", 30),\\\n",
    "       (\"IT\" , 40)\\\n",
    "       ]\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data = dept, schema = deptColumns)\n",
    "deptDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e108d-6da9-4b07-87e1-8faa0eeb2c71",
   "metadata": {},
   "source": [
    "collect() 로 데이터 검색\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3e705eda-27e9-4dbf-8cdb-f576bb83f09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "dataCollect = deptDF.collect()\n",
    "print(dataCollect)\n",
    "#데이터프레임 모든 요소를 드라이버 노드에 로우타입 배열로 반환함,\n",
    "#action이므로 드라이버에 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "223b5b29-5e46-4a9c-bf23-a336fa7c4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance,10\n",
      "Marketing,20\n",
      "Sales,30\n",
      "IT,40\n"
     ]
    }
   ],
   "source": [
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" + str(row['dept_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d45fd927-d511-4348-a384-a366309fb8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finance'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deptDF.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9b2789e-91a5-43a3-bcc7-dec35f471bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finance'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCollect[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f6dc711c-d36e-4833-a1bc-1ffd24cac062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect로 select() \n",
    "dataCollect = deptDF.select(\"dept_name\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d5ab0-0f68-49e4-b6ee-4fc13e532b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0763560e-d224-4a06-9f6f-9ff7aa7411aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|dept_name|\n",
      "+---------+\n",
      "|  Finance|\n",
      "|Marketing|\n",
      "|    Sales|\n",
      "|       IT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select()\n",
    "deptDF.select(\"dept_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac95c52-8003-43b0-9c3c-4f3ecb31e2fd",
   "metadata": {},
   "source": [
    "### collect() vs select()\n",
    "select는 새로운 데이터프레임을 반환하고, 선택된 열을 보유하는 변환\n",
    "collect는 배열의 전체 데이터 세트를 드라이버에 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "25d0447d-3b34-4d72-8992-c8847d82bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "\n",
    "deptDF = spark.createDataFrame(data = dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate = False)\n",
    "\n",
    "dataCollect = deptDF.collect()\n",
    "print(dataCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "51202f64-9f38-4826-ac1f-840a930b6065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\n"
     ]
    }
   ],
   "source": [
    "dataCollect2 = deptDF.select(\"dept_name\").collect()\n",
    "print(dataCollect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9b4e539b-4e91-42e8-b1a2-c1519e1463f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance,10\n",
      "Marketing,20\n",
      "Sales,30\n",
      "IT,40\n"
     ]
    }
   ],
   "source": [
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" + str(row['dept_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e4864-bf9b-473e-ad35-fd458e0c28f4",
   "metadata": {},
   "source": [
    "### withColumn() \n",
    "change the value, convert the datatype of an existing column, create a new column, and many more.\\\n",
    "값을 바꾸거나, 데이터타입을 변환하거나, 새로운 컬럼 만들거나 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4267fc47-0549-4b35-b9e1-a48c92fbdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27da49d6-02e9-4567-b3d7-60875a8c3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dedd62f5-e7aa-4b2b-a0a9-252ee3b17a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data = data, schema = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfac3b-a06c-4c21-8f61-26fa04aca969",
   "metadata": {},
   "source": [
    "change datatype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "687e0285-c422-47f9-ae98-1b350a0ab7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"salary\", col(\"salary\").cast(\"Integer\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a932c85-db46-45a0-8990-a0847d5cff0e",
   "metadata": {},
   "source": [
    "### create column \n",
    "기존 컬럼에서 새로운 컬럼 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2e54e085-9da4-47af-a77f-b0d93f51ae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|CopiedColumn|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       -3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"CopiedColumn\", col(\"salary\")* -1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb5132-f4b7-4c9c-a8d7-71641f417bbd",
   "metadata": {},
   "source": [
    "### add a new column\n",
    "lit()\\\n",
    "lit() 함수는 DataFrame 열에 상수 값을 추가하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c8926f1c-9c15-407b-b0cb-43513cee504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|anotherColumn|\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA| anotherValue|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA| anotherValue|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA| anotherValue|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA| anotherValue|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA| anotherValue|\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Country\", lit(\"USA\")).show()\n",
    "df.withColumn(\"Country\", lit(\"USA\"))\\\n",
    ".withColumn(\"anotherColumn\", lit(\"anotherValue\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c5858-544d-4968-aef5-b3bf214351c4",
   "metadata": {},
   "source": [
    "### rename column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c3717d9f-4e6a-4487-a213-2c8e5897a89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+---+------+\n",
      "|firstname|middlename|lastname|dob       |sex|salary|\n",
      "+---------+----------+--------+----------+---+------+\n",
      "|James    |          |Smith   |1991-04-01|M  |3000  |\n",
      "|Michael  |Rose      |        |2000-05-19|M  |4000  |\n",
      "|Robert   |          |Williams|1978-09-05|M  |4000  |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F  |4000  |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F  |-1    |\n",
      "+---------+----------+--------+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"gender\", \"sex\")\\\n",
    ".show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f52d9-e929-483f-a94b-4cb8cb093b3d",
   "metadata": {},
   "source": [
    "### Drop Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e16231e5-5995-4a41-9ea2-51f9b7004c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|\n",
      "+---------+----------+--------+----------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|\n",
      "|   Robert|          |Williams|1978-09-05|     M|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|\n",
      "+---------+----------+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a037b20-fa62-48da-a1d2-99b6232b9460",
   "metadata": {},
   "source": [
    "### withColumnRenamed\n",
    "withColumnRenamed(existingName, newName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be723083-d38d-4a12-960f-c842a00cd136",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847df145-4853-4b01-a6ee-86a7c865f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25defe6a-a65c-4efa-b801-76c4e73b958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('dob', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b49eeb9-e528-46aa-b430-9e877507a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ca09ef-9983-424a-9c6e-543f2f34cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"dob\", \"DateOfBirth\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8105b0-b64e-4036-9074-a696e30cd31b",
   "metadata": {},
   "source": [
    "### rename multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e1f9621-d679-4032-a5f6-1fc720ed1056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumnRenamed(\"dob\", \"DateOfBirth\").withColumnRenamed(\"salary\", \"salary_amount\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d7d8a-fd75-4d39-b8e4-ef1aee940904",
   "metadata": {},
   "source": [
    "### using Select - rename nested elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e1013e1-6ae7-451a-8b93-62a0e0e5f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- mname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(col(\"name.firstname\").alias(\"fname\"),\\\n",
    "col(\"name.middlename\").alias(\"mname\"),\\\n",
    "col(\"name.lastname\").alias(\"lname\"),\\\n",
    "col(\"dob\"),col(\"gender\"),col(\"salary\"))\\\n",
    ".printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0aec8-8e9c-4129-80c6-d4ea72eac369",
   "metadata": {},
   "source": [
    "### using pyspark DataFrame - rename nested columns\n",
    "기존 컬럼 삭제해야함\\\n",
    "name.firstname => fname 생성, name 삭제\\\n",
    "신규 컬럼은 문자열로, 기존 컬럼은 반드시 컬럼형 (col('컬럼명'))을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa7076a-06ce-415a-ba12-7129e5f89f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- mname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4 = df.withColumn(\"fname\", col(\"name.firstname\"))\\\n",
    ".withColumn(\"mname\", col(\"name.middlename\"))\\\n",
    ".withColumn(\"lname\", col(\"name.lastname\"))\\\n",
    ".drop(\"name\")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d52c2-c6d9-4548-9ff9-028d84119601",
   "metadata": {},
   "source": [
    "### change all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe4616f3-4ba5-4610-a948-78e43c918ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- newCol1: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- newCol2: string (nullable = true)\n",
      " |-- newCol3: string (nullable = true)\n",
      " |-- newCol4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
    "df.toDF(*newColumns).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508501c-ab8a-4e21-bf13-984b8bf50c12",
   "metadata": {},
   "source": [
    "### filter()\n",
    "where() 써도 됨\\\n",
    "조건 만족하는 로우의 새로운 DataFrame or RDD 리턴함\\\n",
    "filter(condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "035d3074-94af-4e57-8815-7b5e74712650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f592b8a6-0071-4596-b31c-3b402fb9a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "    StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3997797-1ab3-467e-91dc-79589c016365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c938e-a458-43ab-9e3a-2ee489401e8a",
   "metadata": {},
   "source": [
    "### DataFram filter() with Column Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a73e29d-3b43-4acd-85c3-84e13ca9ffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state == \"OH\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04d152-3056-40ce-8d7d-ccd20012a4ff",
   "metadata": {},
   "source": [
    "### DataFram filter() with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee72e79a-1fef-46cf-9421-c3f3dd9eda55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+-------------------+------------------+-----+------+\n",
      "|               name|         languages|state|gender|\n",
      "+-------------------+------------------+-----+------+\n",
      "|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "+-------------------+------------------+-----+------+\n",
      "\n",
      "+-------------------+------------------+-----+------+\n",
      "|               name|         languages|state|gender|\n",
      "+-------------------+------------------+-----+------+\n",
      "|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "+-------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"gender == 'M'\").show()\n",
    "df.filter(\"gender != 'M'\").show()\n",
    "df.filter(\"gender <> 'M'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9f8884b-3b4a-498d-95b6-00025788cad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.state == \"OH\") & (df.gender == \"M\") ).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ee207-b31d-464e-b04c-0559407cb3b6",
   "metadata": {},
   "source": [
    "### Filter List Values\n",
    "isin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc27265-aea3-408f-a33d-28865e43349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "li = [\"OH\", \"CA\", \"DE\"]\n",
    "df.filter(df.state.isin(li)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f561ec6a-5375-4f4e-893f-1fcacf7fe2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~df.state.isin(li)).show()\n",
    "df.filter(df.state.isin(li)==False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13132596-d248-469d-8a70-24493baa0432",
   "metadata": {},
   "source": [
    "### Starts With, Ends With, Contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d34095c-45ea-4f7a-a58a-08ca166b768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.startswith(\"N\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6ab6d3-26a5-46a1-b559-5bca44b90d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.endswith(\"H\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10246108-7e4c-4de6-8218-f9fb36395375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.state.contains(\"H\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4674ee-e309-4f90-bb02-4a8e6bd75660",
   "metadata": {},
   "source": [
    "### Filter like and rlike\n",
    "rlike(regex like) 대소문자 구분하지 않고 필터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ab8d35-91c4-4719-a354-013a774b8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col, array_contains\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4364d7a-a617-40bb-b29b-fc2a7b92f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\\\n",
    "     (4,\"Rames Rose\"),(5,\"Rames rose\")\n",
    "  ]\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62f668a-a5ce-49a0-aea9-0e2c29318be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      name|\n",
      "+---+----------+\n",
      "|  5|Rames rose|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(df2.name.like(\"%rose%\")).show()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b3b37b-11ec-4bd0-9cf0-9c4d7e259195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|        name|\n",
      "+---+------------+\n",
      "|  2|Michael Rose|\n",
      "|  4|  Rames Rose|\n",
      "|  5|  Rames rose|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04782632-9db6-4db1-9031-67a7cac7462b",
   "metadata": {},
   "source": [
    "### filter Array column\n",
    "array_contains()\\\n",
    "값이 배열에 포함되어 있으면 true 반환, 아니면 false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7b1c841-ffa0-4f8c-b531-f9e3369ab040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+------+\n",
      "|name            |languages         |state|gender|\n",
      "+----------------+------------------+-----+------+\n",
      "|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |\n",
      "+----------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df.filter(array_contains(df.languages, \"Java\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcaf14-80a1-4562-a9c1-c218e09b6f3d",
   "metadata": {},
   "source": [
    "### Filtering on Nested Struct columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b251e49-3071-44d1-9ca5-16d84f92744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------+-----+------+\n",
      "|name                  |languages   |state|gender|\n",
      "+----------------------+------------+-----+------+\n",
      "|{Julia, , Williams}   |[CSharp, VB]|OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]|OH   |M     |\n",
      "+----------------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.lastname == \"Williams\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff978f-d9f3-4d8f-9861-909a06fac67e",
   "metadata": {},
   "source": [
    "multiple column filter \\\n",
    "and => &, or => |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93404a56-871a-49f7-8d4b-d15bd2a9fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter( (df.state == \"OH\") & (df.gender == \"M\") ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20350d48-494f-4430-85f5-1aaf63a3178b",
   "metadata": {},
   "source": [
    "### Distinct Duplicate row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f71672-5907-407b-aa2b-76a4b856f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fecfef77-e74a-4be2-8d3c-a2fdaf57b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "# Prepare Data\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e0080e0-9e5b-49e0-9844-85a12a310e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19908d31-9a78-4b58-b0dc-0c692fc8d456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62648856-5424-45d2-ad93-945553e473ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test 내가 해본 예제\n",
    "df.createOrReplaceTempView(\"tempview\")\n",
    "spark.sql('select * from tempview').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e3f49-febf-4255-b6ff-426c0e3eb6c9",
   "metadata": {},
   "source": [
    "### Distinct row\n",
    "중복제거하고 남은 로우 출력\\\n",
    "모든행을 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0eb3b6c-50ac-4ea1-a6a2-efed9f5cfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctDF = df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1de211f-79ed-4205-91fb-6a050ca2182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count:9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Michael      |Sales     |4600  |\n",
      "|James        |Sales     |3000  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Distinct count:\" + str(distinctDF.count()))\n",
    "distinctDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9bcd422-b4cf-44f5-aa3d-d52d3d9e481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count:9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Michael      |Sales     |4600  |\n",
      "|James        |Sales     |3000  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.dropDuplicates()\n",
    "print(\"Distinct count:\" + str(df2.count()))\n",
    "df2.show(truncate = False)\n",
    "#모든 행을 비교해서 이름, 부서, 연봉이 다 겹쳤을 때 제거함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c0302-fa0d-4a4a-b256-e99827a9534c",
   "metadata": {},
   "source": [
    "### Distinct of selected Multiple Columns\n",
    "dropDuplicates() multiple columns to eliminate duplicates.\\\n",
    "dropDuplicates() on DataFrame returns a new DataFrame with duplicate rows removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b40ee9-ccb7-4af9-99e7-3744e713e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count of department & salary : 8\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Michael      |Sales     |4600  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropDisDF = df.dropDuplicates([\"department\", \"salary\"])\n",
    "print(\"Distinct count of department & salary : \" + str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate = False)\n",
    "\n",
    "# 선택한 두 컬럼만 비교해서 중복을 제거함, 이름은 같던 말던~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbcafc-d5fe-4d74-8f80-e004127653ab",
   "metadata": {},
   "source": [
    "### orderBy() sort()\n",
    "sorting 해서 새로운 DataFrame 리턴함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532cd357-9b67-4394-8c4a-32ad5a74e304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc, asc\n",
    "\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88e99e-08bc-430e-85d6-8b9e0c3fef4b",
   "metadata": {},
   "source": [
    "### sorting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2d5657b-7cb4-4bb9-be18-666d57b3c7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_name: string, department: string, state: string, salary: bigint, age: bigint, bonus: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort(\"salary\" , \"age\", ascending = [True, False])\n",
    "# column1 은 ascending 오름차순, column2 는 descending 내림차순"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55f1c07f-dd23-4c92-9b81-f5876ed1ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"department\", \"state\").show(truncate = False)\n",
    "df.sort(col(\"department\"), col(\"state\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7fb4b-64da-4d59-9ce4-c4642ef29962",
   "metadata": {},
   "source": [
    "### orderBy()\n",
    "By default, it orders by ascending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2622787b-7607-497f-beef-d8ff5221e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"department\", \"state\").show(truncate = False)\n",
    "df.orderBy(col(\"department\"), col(\"state\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b267ab-eede-4fa4-8ff9-8c549ab007d1",
   "metadata": {},
   "source": [
    "### sort by asc\n",
    "명시적으로 asc 넣고싶으면 asc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97716d8c-4e2f-4356-b6be-8074496962a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.department.desc(), df.state.desc()).show(truncate = False)\n",
    "df.sort(col(\"department\").asc(), col(\"state\").asc()).show(truncate = False)\n",
    "df.orderBy(col(\"department\").asc(), col(\"state\").asc()).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8375b-2343-43f8-9766-38bc63e1a640",
   "metadata": {},
   "source": [
    "### sort by desc\n",
    "desc(), asc_nulls_first() and asc_nulls_last() and equivalent descending functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93290464-a407-417c-bf61-a12e0e6c00e9",
   "metadata": {},
   "source": [
    "### Raw SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e16c235-7d73-45bb-b438-a08cb9cb2787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select employee_name, department, state, salary, age, bonus from EMP ORDER BY department asc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe80f8-0fe7-4e28-8ad5-210e01dfbc0b",
   "metadata": {},
   "source": [
    "### groupBy() \n",
    "to collect the identical data into groups on DataFrame \\\n",
    "and perform count, sum, avg, min, max functions on the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d50a1625-60f7-4455-be90-73cb8a5f3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b7b7622-09ad-4154-9f39-7b7d40298c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data = simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d3e0c-1ea6-40fa-8361-55a243335e58",
   "metadata": {},
   "source": [
    "### groupBy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3465b53-a2e9-43b1-a84f-a3367c690315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "496ee3d2-b395-4b8e-8590-b83059239db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03395b4a-5fee-4842-b409-8a612c56e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |81000      |\n",
      "|Finance   |79000      |\n",
      "|Marketing |80000      |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").min(\"salary\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1002239-0dd9-40e0-8f15-70d2dfb30e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      90000|\n",
      "|   Finance|      99000|\n",
      "| Marketing|      91000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69b98bc0-d360-4f10-a5a1-50ddf24cd5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").avg(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "134c4dff-c291-45a5-89d7-9890648a5d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mean도 평균\n",
    "df.groupBy(\"department\").mean(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be76835-3fec-4e44-9fd2-8a726b975dc0",
   "metadata": {},
   "source": [
    "### groupBy Multiple columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "952ccf72-bae6-4754-9f65-fcb59f8ec5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|Finance   |CA   |189000     |47000     |\n",
      "|Sales     |NY   |176000     |30000     |\n",
      "|Finance   |NY   |162000     |34000     |\n",
      "|Marketing |NY   |91000      |21000     |\n",
      "|Sales     |CA   |81000      |23000     |\n",
      "|Marketing |CA   |80000      |18000     |\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\", \"state\")\\\n",
    ".sum(\"salary\", \"bonus\").sort(desc(\"sum(salary)\"))\\\n",
    ".show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca26ce2-8d2b-4bab-b123-23529f03c9ed",
   "metadata": {},
   "source": [
    "### aggregates at a time\n",
    "agg()를 이용해서 집계쿼리 sum(), avg(), min(), max() mean() e.t.c. 한 문장으로 \\\n",
    "\"from pyspark.sql.functions import sum,avg,max,min,mean,count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "214c47f5-8da7-40d6-a2d2-8497eeee4ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, max\n",
    "df.groupBy(\"department\")\\\n",
    ".agg(sum(\"salary\").alias(\"sum_salary\"),\\\n",
    "avg(\"salary\").alias(\"avg_salary\"),\\\n",
    "sum(\"bonus\").alias(\"sum_bonus\"),\\\n",
    "max(\"bonus\").alias(\"max_bonus\")\\\n",
    "    )\\\n",
    ".show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9eb93c-9272-419a-b9fc-ec14d981d3f6",
   "metadata": {},
   "source": [
    "### filter on aggregate data\n",
    "similar to HAVING\\\n",
    "use either where() or filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebf40fb-f833-4897-8a89-ff7d3680a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, max\n",
    "\n",
    "df.groupBy(\"department\")\\\n",
    ".agg(sum(\"salary\").alias(\"sum_salary\"),avg(\"salary\").alias(\"avg_salary\"),\\\n",
    "sum(\"bonus\").alias(\"sum_bonus\"),\\\n",
    "max(\"bonus\").alias(\"max_bonus\"))\\\n",
    ".where(col(\"sum_bonus\") >= 50000)\\\n",
    ".show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555f6e3-f499-49d1-ad49-54e20b263a4b",
   "metadata": {},
   "source": [
    "### Join Two DataFrames\n",
    "support all basic join type(INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN)\\\n",
    "involve data suffling across the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbd753-060a-43b7-bf47-783fa594152f",
   "metadata": {},
   "source": [
    "join(self, other, on = None, how=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdea9cd6-8dac-437a-b335-409c154be2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data = emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate = False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data = dept, schema = deptColumns)\n",
    "deptDF.show(truncate = False)\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956de66d-f6a5-47b6-b999-1f6c13a8bd6f",
   "metadata": {},
   "source": [
    "join operation works by combining data from two or more Datasets based on a common column or key\\\n",
    "\n",
    "common key: 조인하기 위해서는 common key or column 필요함. 이 key 는 일치하는 row를 조인하기 위해 사용되어짐\\\n",
    "partitioning: pyspark 데이터셋은 여러 노드에 분산,분할 되어있음. 이상적으로 같은 조인 키 데이터는 같은 파티션에 위치되어 있어야함\\\n",
    "데이터셋이 조인키로 분할되지 않은 경우 셔플을 수행해서 데이터를 재분산하고 동일 키를 가진 로우를 동일 노드에 있도록 할 수 있음\\\n",
    "셔플링은 비싼 작업임, 특히 큰 데이터셋에서\n",
    "\n",
    "inner join: 두 데이터프레임에서 같은 키를 가진 로우만 반환\\\n",
    "left join: 왼쪽 데이터프레임의 모든 로우와 오른쪽 데이터프레임과 매칭되는 로우를 반환함\\\n",
    "right join: 오른쪽 데이터프레임의 모든 로우와 왼쪽 데이터프레임과 매칭되는 로우 반환\\\n",
    "full outer join: 두 데이터프레임의 모든 로우 반환\\\n",
    "left semi join: 오른쪽 데이터프레임과 매치되는 왼쪽 데이터프레임 모든 로우 반환\\\n",
    "left anti join: 오른쪽 데이터프레임과 매치되지 않는 왼쪽 데이터프레임 모든 로우 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a80cdf3-5853-432c-b21a-545185ca49af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inner join (defalut)\n",
    "#inner join combines two DataFrames based on the key(common column) provided and results in rows \n",
    "\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"inner\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ccb1258-4815-49e9-8502-c5801707398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full outer join\n",
    "#모든 행 반환, 조인 식이 일치하지 않는 경우 null 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610dacde-d9af-430a-b7a5-7f872a85886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"outer\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78614e96-627e-47fa-b15c-d34a48f1d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"full\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb1e477-0a2a-454d-900d-2f0737bbbe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"fullouter\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fae2338-8df0-4eff-826a-2a053b1593a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Left outer join\n",
    "#\n",
    "empDF.show()\n",
    "deptDF.show()\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"left\").show(truncate = False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftouter\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e8c193-74df-43b1-baf2-3f343432753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |6     |Brown   |2              |2010       |50         |      |-1    |null     |null \n",
    "# emp_dept_id에서 dept_id와 일치하지 않는 Brown에 대해 deptDF 컬럼에 null 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ebef9c8-7147-43c7-bde3-aa9be357b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#right outer join\n",
    "empDF.show()\n",
    "deptDF.show()\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"right\").show(truncate = False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"rightouter\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "056e50fe-0ee8-4e8a-afd9-b0a524b92f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dept_id 와 일치하는 값의 row 다 반환\n",
    "# 일치하지 않는 empDF row 값에 대해 null 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb639fbb-cda7-4566-abc9-cf4822151760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#left semi join\n",
    "#inner join이랑 비슷함. 왼쪽 데이터셋 모든 컬럼 리턴하고 오른쪽은 무시함,\n",
    "#오른쪽 데이터프레임과 매치되는 레코드 & 왼쪽 데이터프레임 컬럼만 리턴\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftsemi\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3ac7722-6e45-4b2e-9f6e-4b7b77b2f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#leftanti\n",
    "#lefts semi 의 반대 \n",
    "#매치되는 않는 왼쪽 데이터셋만 레코드만 리턴\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftanti\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90d3c3-6ae5-40c0-a12b-073392e15a75",
   "metadata": {},
   "source": [
    "### Self join\n",
    "위에 있는 조인 유형 선택해서 셀프조인 사용할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b41aa72-cdb8-42ed-98f0-161d535af6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "==self join==\n",
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|2     |Rose    |1              |Smith            |\n",
      "|3     |Williams|1              |Smith            |\n",
      "|4     |Jones   |2              |Rose             |\n",
      "|5     |Brown   |2              |Rose             |\n",
      "|6     |Brown   |2              |Rose             |\n",
      "+------+--------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.show()\n",
    "print(\"==self join==\")\n",
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"),\\\n",
    "col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"), \"inner\")\\\n",
    ".select(col(\"emp1.emp_id\"),col(\"emp1.name\"),\\\n",
    "col(\"emp2.emp_id\").alias(\"superior_emp_id\"),\\\n",
    "col(\"emp2.name\").alias(\"superior_emp_name\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4230dadd-fbf4-4405-b021-7b557102284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|2     |Rose    |1              |Smith            |\n",
      "|3     |Williams|1              |Smith            |\n",
      "|4     |Jones   |2              |Rose             |\n",
      "|5     |Brown   |2              |Rose             |\n",
      "|6     |Brown   |2              |Rose             |\n",
      "+------+--------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"),col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"), \"inner\")\\\n",
    ".select(col(\"emp1.emp_id\"),col(\"emp1.name\"),col(\"emp2.emp_id\").alias(\"superior_emp_id\"),col(\"emp2.name\").alias(\"superior_emp_name\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20940e33-61b9-4fbc-ab85-a02f97664c16",
   "metadata": {},
   "source": [
    "### SQL expression\n",
    "\n",
    "<!-- empDF.createOrReplaceTempView(\"EMP\")\n",
    "detDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\").show(truncate = False)\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b0157d6-a206-44f7-993a-1af9031e4351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\").show(truncate = False)\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b30a22-74f9-43e4-a851-6c8a0c7d8fd1",
   "metadata": {},
   "source": [
    "### Join on multiple DataFrames\n",
    "join more than two tables, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f230c-86b2-42d9-9a08-3cc9772d504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, df1.id1 == df2.id2, \"inner\").join(df3, df1.id1 == df3.id3, \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36a1af-c343-467a-9e06-0871f0d8d042",
   "metadata": {},
   "source": [
    "### union and union all\n",
    "\n",
    "union(): merge two DataFrame's of the same structure/schema. schemas are not the same it returns an error \\\n",
    "unionAll(): deprecated since Spark 2.0.0, replace with union()\n",
    "\n",
    "In SQL, union eliminates duplicates but unionall merge two datasets including duplicate records\\\n",
    "but, in pyspark recommend duplicate() function to remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0317ef97-f398-4e5a-92e1-fb6e17269c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d9f3766-7347-4403-ac36-36b8c624f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab11c7-3dc5-4237-bc17-94c04b5e1a88",
   "metadata": {},
   "source": [
    "### Merge two or more DataFrames using union\n",
    "merges two DataFrames and returns the new DataFrame \\\n",
    "with all rows from two Dataframes regardless of duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99d0501a-2ff3-451f-a4ef-9c680ccaf6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate = False)\n",
    "\n",
    "#same output\n",
    "unionAllDF = df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5b0f2-80a2-432c-8119-10ec9befad7d",
   "metadata": {},
   "source": [
    "### Merge without Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a82a1f2f-6e5f-4175-8fd4-a0b88068b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f02577-3a8a-4073-aa56-0527adbce00d",
   "metadata": {},
   "source": [
    "### unionByName()\n",
    "merge/union two DataFrames with column names\\\n",
    "열 이름으로 두 데이터프레임 병합하는데 사용\\\n",
    "열 이름이 다른 순서로 있거나 DataFrame에 누락된 열이 있는 경우 두 개의 DataFrame 통합하는데 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f1b2c-2039-4d62-a813-6f62214a9b86",
   "metadata": {},
   "source": [
    "unionByName(df, allowMissingColumns = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d035bb-e646-4cfa-807d-382fc9575ac3",
   "metadata": {},
   "source": [
    "### Difference between unionByName() vs union()\n",
    "unionByName() is used to merge two DataFrames by column names instead of by position\\\n",
    "unionByName() also provides an argument allowMissingColumns to specify if you have a different column counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d00d05dd-4a2e-42da-8adf-9e7364a00d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [(\"James\",34), (\"Michael\",56), \\\n",
    "        (\"Robert\",30), (\"Maria\",24) ]\n",
    "df1 = spark.createDataFrame(data = data, schema = [\"name\", \"id\"])\n",
    "df1.printSchema()\n",
    "\n",
    "data2=[(34,\"James\"),(45,\"Maria\"), \\\n",
    "       (45,\"Jen\"),(34,\"Jeff\")]\n",
    "\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\", \"name\"])\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6ecb613-9abb-4664-9959-216dde47cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  James| 34|\n",
      "|Michael| 56|\n",
      "| Robert| 30|\n",
      "|  Maria| 24|\n",
      "|  James| 34|\n",
      "|  Maria| 45|\n",
      "|    Jen| 45|\n",
      "|   Jeff| 34|\n",
      "+-------+---+\n",
      "\n",
      "+-------+-----+\n",
      "|   name|   id|\n",
      "+-------+-----+\n",
      "|  James|   34|\n",
      "|Michael|   56|\n",
      "| Robert|   30|\n",
      "|  Maria|   24|\n",
      "|     34|James|\n",
      "|     45|Maria|\n",
      "|     45|  Jen|\n",
      "|     34| Jeff|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.unionByName(df2)\n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "\n",
    "df3t = df1.union(df2)\n",
    "df3t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a635fa4-588c-49c5-a0c4-ee4c746c793c",
   "metadata": {},
   "source": [
    "### unionByName() with Different Number of Columns\n",
    "different number of columns then use allowMissingColumns = True\\\n",
    "the result of the DataFrame contains null values for the columns that are missing on the DataFrame\n",
    "\n",
    "#### param allowMissingColumns is available since Spark 3.1 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7a5b461-fae0-498e-8f0a-d304e8d05aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col0: long (nullable = true)\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      " |-- col3: long (nullable = true)\n",
      "\n",
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|   5|   2|   6|null|\n",
      "|null|   6|   7|   3|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "df3 = df1.unionByName(df2, allowMissingColumns = True)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144c856-f2cf-4ffd-a03d-22bf9054b97b",
   "metadata": {},
   "source": [
    "### UDF\n",
    "UDF’s are the most expensive operations hence use them only you have no choice and when essential\\\n",
    "선택의 여지가 없고, 꼭 필요할때만 사용해라!\n",
    "\n",
    "create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively\n",
    "\n",
    "기능 재사용 시 사용. \bex)첫번째 글자를 대문자로 전환할때 udf 만들어서 재사용 할 수 있음\\\n",
    "udf 만들기 전에 비슷한 기능이 있는지 확인하는 게 좋음, 그리고 디자인을 매우 세심하게 해야함 그렇지 않으면 성능 및 최적화 문제 발생할 가능성 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f4c93-c9c5-4f16-bba2-c3813e751610",
   "metadata": {},
   "source": [
    "### create UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc639870-d504-45ee-ae46-cdc427539788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8bd99f-f5d8-43c9-abaf-fb0b02790e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resStr = \"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "        resStr = resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86a18b1-cca5-42c1-9371-3e332544acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# StringType() is by default hence not required 원래 스트링이 기본이니까 옵션 안줘도됨\n",
    "convertUDF = udf(lambda z: convertCase(z), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd8a80-22fe-4c66-997a-363ebec6cc57",
   "metadata": {},
   "source": [
    "### UDF with DataFrame select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef3ee45-730f-452e-96a5-b7b9cfbc19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Seqno\"),convertUDF(col(\"Name\")).alias(\"Name\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d2e75-f9ab-4e04-aade-e0e01f99d34c",
   "metadata": {},
   "source": [
    "### UDF with DataFrame withColumn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d73b40a-f310-427c-a3bc-bf10753c1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperCase(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b00610d-e50f-40a0-854d-9a46a18b73b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upperCaseUDF = udf(lambda z:upperCase(z), StringType())\n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750c4bb-f8d9-4ecb-9035-e8a944d9e5ec",
   "metadata": {},
   "source": [
    "### UDF & use it on SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66b3f98-d3e9-4fd9-9d16-7d91bbe019d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"convertUDF\", convertCase, StringType())\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c81030-9d6f-41f3-ada8-6ffb009d82df",
   "metadata": {},
   "source": [
    "### UDF using annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1f794c-da1e-464d-a08b-facd220f5659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType = StringType())\n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "df.withColumn(\"Cureated Name\",  upperCase(col(\"Name\"))).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616fade-1274-4591-9d8f-5bc344818b49",
   "metadata": {},
   "source": [
    "### Excution order\n",
    "실행순서를 보장하지 않음\\\n",
    "pyspark 는 쿼리 최적화 및 Planning을 위해 실행순서를 재정렬 하므로 표현식을 사용할 때 부작용을 주의해야함\\\n",
    "UDF 사용 시에는 null handling을 특히 주의해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "348cc80e-94e4-422b-b19d-3aee14131bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "No guarantee Name is not null will execute first\n",
    "If convertUDF(Name) like '%John%' execute first then \n",
    "you will get runtime error\n",
    "\"\"\"\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654382b7-6c15-4137-9937-b0fec6d02f40",
   "metadata": {},
   "source": [
    "### Handling null check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5674265f-ec36-46a1-9a56-acfcc992e92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |null        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_124/3922637301.py\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m df2\u001b[38;5;241m.\u001b[39mshow(truncate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m df2\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME_TABLE2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect convertUDF(Name) from NAME_TABLE2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:912\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    905\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    906\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m         },\n\u001b[1;32m    910\u001b[0m     )\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_124/3922637301.py\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "\"\"\" null check \"\"\"\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data = data, schema = columns)\n",
    "df2.show(truncate = False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "\n",
    "spark.sql(\"select convertUDF(Name) from NAME_TABLE2\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2fc2550-9d74-4c90-922b-e86012e6d1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|_nullsafeUDF(Name)|\n",
      "+------------------+\n",
      "|John Jones        |\n",
      "|Tracey Smith      |\n",
      "|Amy Sanders       |\n",
      "|                  |\n",
      "+------------------+\n",
      "\n",
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\",StringType())\n",
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\").show(truncate = False)\n",
    "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
    "          \"where Name is not null and _nullsafeUDF(Name) like '%John%'\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9e0cc-5e9e-425c-8650-4864e583ebdd",
   "metadata": {},
   "source": [
    "### transform()\n",
    "pyspark provides two transform() functions one with DataFrame and another in pyspark.sql.function\n",
    "\n",
    "pyspark.sql.DataFrame.transform() – Available since Spark 3.0\\\n",
    "pyspark.sql.functions.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab184a10-458f-4351-85ed-df742226732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      "\n",
      "+----------+----+--------+\n",
      "|CourseName|fee |discount|\n",
      "+----------+----+--------+\n",
      "|Java      |4000|5       |\n",
      "|Python    |4600|10      |\n",
      "|Scala     |4100|15      |\n",
      "|Scala     |4500|15      |\n",
      "|PHP       |3000|20      |\n",
      "+----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44478095-4f6c-46dc-8c3e-74f30fec158d",
   "metadata": {},
   "source": [
    "### DataFrame.transform()\n",
    "pyspark.sql.DataFrame.transform()\n",
    "사용자 함수를 연결하고 새로운 DataFrame()을 리턴함\n",
    "\n",
    "#### DataFrame.transform(func: Callable[[..], DataFrame], *args: Any,**kwargs: Any) -> pyspark.sql.dataframe.DataFrame\n",
    "func - custom function to call\\\n",
    "*args - Arguments to pass to func\\\n",
    "*kwargs - Keyword arguments to pass to func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22099783-cd0d-45ae-92c1-5ef70d59d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformation 1\n",
    "from pyspark.sql.functions import upper\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"CourseName\", upper(df.CourseName))\n",
    "\n",
    "# Custom transformation 2\n",
    "def reduce_price(df, reduceBy):\n",
    "    return df.withColumn(\"new_fee\", df.fee - reduceBy)\n",
    "\n",
    "# Custom transformation 3\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discounted_fee\",df.new_fee - (df.new_fee * df.discount) / 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee45b10-1a24-4128-b58a-e8d84513d8ca",
   "metadata": {},
   "source": [
    "### Apply DataFrame.transform()\n",
    "사용자 정의 함수를 연결하고 transform() 함수를 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e365488-9db6-444e-8a33-09fcb59e1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.transform(to_upper_str_columns).transform(reduce_price, 1000).transform(apply_discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9b0846d-4b08-4110-a416-4776658d9a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName|fee |discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|JAVA      |4000|5       |3000   |2850.0        |\n",
      "|PYTHON    |4600|10      |3600   |3240.0        |\n",
      "|SCALA     |4100|15      |3100   |2635.0        |\n",
      "|SCALA     |4500|15      |3500   |2975.0        |\n",
      "|PHP       |3000|20      |2000   |1600.0        |\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac029b38-2e30-4925-8b45-ef2c3486b287",
   "metadata": {},
   "source": [
    "열을 선택하려는 경우 select()를 쓰거나, 또 다른 custom function을 쓸 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5bc6d9d-5eba-4570-82da-8cc72e2d4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function\n",
    "def select_columns(df):\n",
    "    return df.select(\"CourseName\", \"discounted_fee\")\n",
    "    \n",
    "#chain transformations\n",
    "df2 = df.transform(to_upper_str_columns).transform(reduce_price,1000).transform(apply_discount).transform(select_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7fdf908-4e18-4c1e-847d-21684961e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|CourseName|discounted_fee|\n",
      "+----------+--------------+\n",
      "|JAVA      |2850.0        |\n",
      "|PYTHON    |3240.0        |\n",
      "|SCALA     |2635.0        |\n",
      "|SCALA     |2975.0        |\n",
      "|PHP       |1600.0        |\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241548e1-4986-4e69-ae31-5d2063917d32",
   "metadata": {},
   "source": [
    "### sql.functions.transform()\n",
    "apply the transformation on a column of type Array. This function applies the specified transformation on every element of the array and returns an object of ArrayType.\n",
    "\n",
    "배열 컬럼 변형에 적용됨. 이 함수는 모든 요소에 지정된 변환을 적용하고 배열타입 객체를 리턴함\n",
    "#### pyspark.sql.functions.transform(col, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0156071-5f70-4709-9537-5a3c6f48d34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Languages1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Languages2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+----------------+------------------+---------------+\n",
      "|            Name|        Languages1|     Languages2|\n",
      "+----------------+------------------+---------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
      "+----------------+------------------+---------------+\n",
      "\n",
      "+------------------+\n",
      "|        languages1|\n",
      "+------------------+\n",
      "|[JAVA, SCALA, C++]|\n",
      "|[SPARK, JAVA, C++]|\n",
      "|      [CSHARP, VB]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "]\n",
    "df = spark.createDataFrame(data = data, schema = [\"Name\", \"Languages1\", \"Languages2\"])\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.functions import transform\n",
    "df.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b020e-6d6c-4e73-a180-3967f5952c3b",
   "metadata": {},
   "source": [
    "### apply Function to Column\n",
    "withColumn(), sql(), select() 사용해서 기존 함수나 custom 함수 컬럼에 적용할 수 있음\\\n",
    "custom function에 적용하기 위해 function을 만들고 udf로 등록해야함.\\\n",
    "pyspark 최신 버전에서는 pandas API 사용을 제공하므로 pyspark.pandas.DataFrame.apply() 사용할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57166088-0722-4c24-93e5-4bc1932d56d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "columns = [\"Seqno\", \"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51718340-ad5f-4014-bd31-932f5453a122",
   "metadata": {},
   "source": [
    "### apply Function using withColumn()\n",
    "withColumn() is a transformation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "202a5cc0-29e0-415d-9dfb-538bc9c1272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|  Upper_Name|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "df.withColumn(\"Upper_Name\", upper(df.Name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a9fdb-7d33-4551-9594-7f006cccba2a",
   "metadata": {},
   "source": [
    "### Apply Function using select()\n",
    "select() is used to select the columns from the pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d40938c-9307-45d6-91d4-5f121219349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name| upper(Name)|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Seqno\", \"Name\", upper(df.Name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d33f6-8519-49aa-8d63-8d18028dfc93",
   "metadata": {},
   "source": [
    "### apply function using SQL\n",
    "SQL 쿼리 실행하기 위해서는 spark.sql() function , createOrReplaceTempView() 사용해야함\\\n",
    "이 테이블은 SparkSession 끄기 전까지 사용가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738af583-7f1e-48ed-85ab-1309ba204c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name| upper(Name)|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select Seqno, Name, UPPER(Name) from TAB\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe5abe-6263-4501-91c9-2976958496a7",
   "metadata": {},
   "source": [
    "### Custom Function\n",
    "upper()은 있는 함수지만 예시를 위해 만듦, udf는 비싼 작업이니까 선택의 여지가 없을 때 사용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e73fdac-87c7-4965-9d6c-d60fd384edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperCase(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9648216-c433-47d9-b2c6-7557124498e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "upperCaseUDF = udf(lambda x: upperCase(x), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4a677-43ae-4d56-842a-60c51dc5a827",
   "metadata": {},
   "source": [
    "### Apply Custom UDF to Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0470b27a-ed4b-4639-9db6-b67973b03afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n",
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |JOHN JONES  |\n",
      "|2    |TRACEY SMITH|\n",
      "|3    |AMY SANDERS |\n",
      "+-----+------------+\n",
      "\n",
      "+-----+------------+------------------+\n",
      "|Seqno|        Name|upperCaseUDF(Name)|\n",
      "+-----+------------+------------------+\n",
      "|    1|  john jones|        JOHN JONES|\n",
      "|    2|tracey smith|      TRACEY SMITH|\n",
      "|    3| amy sanders|       AMY SANDERS|\n",
      "+-----+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))).show(truncate = False)\n",
    "df.select(col(\"Seqno\"),upperCaseUDF(col(\"Name\")).alias(\"Name\")).show(truncate = False)\n",
    "spark.udf.register(\"upperCaseUDF\", upperCaseUDF)\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select Seqno, Name, upperCaseUDF(Name) from TAB\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994adfdc-bb88-454f-9004-b0d89399d9cc",
   "metadata": {},
   "source": [
    "### Pandas apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d17f3a5a-9b26-4d42-8e28-a15a35ad6b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Fee  Discount\n",
      "0  20000.0      1000\n",
      "1  25000.0      2500\n",
      "2  30000.0      1500\n",
      "3  22000.0      1200\n",
      "4      NaN      3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21000.0\n",
      "1    27500.0\n",
      "2    31500.0\n",
      "3    23200.0\n",
      "4        NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "\n",
    "technologies = ({\n",
    "    'Fee' :[20000,25000,30000,22000,np.NaN],\n",
    "    'Discount':[1000,2500,1500,1200,3000]\n",
    "               })\n",
    "\n",
    "psdf = ps.DataFrame(technologies)\n",
    "print(psdf)\n",
    "\n",
    "def add(data):\n",
    "    return data[0] + data[1]\n",
    "    \n",
    "addDF = psdf.apply(add, axis=1)\n",
    "print(addDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9c120-2466-443e-ba60-a26aae1e8e0d",
   "metadata": {},
   "source": [
    "축(axis)는 n차원 배열을 구성하는 요소\\\n",
    "axis = n으로 불리는 축은 그냥 바깥 리스트에서 안쪽 리스트 순으로 0부터 이름 붙인것에 불과\\\n",
    "1차원 배열은 axis=0\\\n",
    "2차원 배열은 axis=0,axis=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fcf90-ac8a-4ad1-8265-4a58ca3821c4",
   "metadata": {},
   "source": [
    "### map() Transformation\n",
    "RDD transformation. RDD/DataFrame 의 모든요소에 변환함수를 적용하고 새로운 RDD를 반환\\\n",
    "컬럼 추가, 컬럼 업데이트, data 변환 작업 등에 적용됨\\\n",
    "map 변환에 결과는 항상 입력 레코드 수와 동일함\n",
    "#### Note1: DataFrame doesn’t have map() transformation to use with DataFrame hence you need to convert DataFrame to RDD first.\\\n",
    "데이터프레임은 map() 없음 -> DatafFrame 을 RDD 로 변환\n",
    "#### Note2: If you have a heavy initialization use PySpark mapPartitions() transformation instead of map(), as with mapPartitions() heavy initialization executes only once for each partition instead of every record.\n",
    "초기화가 많은 경우 mapPartitions()를 사용, 모든 레코드 대신 각 파티션에 대해 한 번만 실행됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c5cf45-4f74-479b-a3a7-f6f0ef103ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa1c06-e06a-4d3f-81e7-4f97b2b09f23",
   "metadata": {},
   "source": [
    "#### map(f, preservesPartitioning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c510529-1179-4edc-8cc7-931ee4fe136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x:(x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e815a0d-7c8e-499d-84a2-a17719faa9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|    30|\n",
      "|     Anna|    Rose|     F|    41|\n",
      "|   Robert|Williams|     M|    62|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70346625-dfe2-48c2-aa75-80c193dc7071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#referring column by index\n",
    "rdd2 = df.rdd.map(lambda x:(x[0]+\",\"+x[1], x[2], x[3]*2))\n",
    "df2 = rdd2.toDF([\"name\", \"gender\", \"new_salary\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99ac852-02ce-436b-9b71-906999a47ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column Names\n",
    "rdd2 = df.rdd.map(lambda x:(x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df430e1b-26c0-49fa-98ba-de795d6a1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = df.rdd.map(lambda x:(x.firstname+\",\"+x.lastname, x.gender, x.salary*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a16ebe4-d02f-4bfd-88b1-46f2a5390915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    firstName = x.firstname\n",
    "    lastName = x.lastname\n",
    "    name = firstName + \",\"+ lastName\n",
    "    gender=x.gender.lower()\n",
    "    salary = x.salary*2\n",
    "    return(name,gender,salary)\n",
    "rdd2 = df.rdd.map(lambda x: func(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456dcfd3-0496-4d44-986c-b7b0a2cdd371",
   "metadata": {},
   "source": [
    "### flatMap() transformation\n",
    "flattens the RDD/DataFrame(array/map/DataFrame columns) after applying the function on every element and return a new PySpark RDD/DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "495d7a86-4a94-42a4-a4d1-d4700c302e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\n",
      "Alice’s Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Adventures in Wonderland\n",
      "Project Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b73cb-013b-4072-9161-4c14c71009b7",
   "metadata": {},
   "source": [
    "#### flatMap(f, preservesPartitioning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c6cf3e4-935d-463a-a6e8-1680e588283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Gutenberg’s\n",
      "Alice’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbfe7969-0f24-4f69-8568-da6b6553556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\n",
      "Alice’s Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Project\n",
      "Gutenberg’s\n",
      "Alice’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d9ac7f-35a2-4643-89e9-22927dcb96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  null|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
    "\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df = spark.createDataFrame(data = arrayData, schema = ['name', 'knownLanguages','properties'])\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2bf5a-0592-4681-a910-7d36d4053688",
   "metadata": {},
   "source": [
    "### foreach()\n",
    "available RDD, DataFrame to iterate/loop over each element in the DataFrame,\\\n",
    "similar to for with advanced concepts\n",
    "#### DataFrame.foreach(f)\n",
    "\n",
    ".This operation is mainly used if you wanted to manipulate accumulators, save the DataFrame results to RDBMS tables, Kafka topics, and other external sources.\\\n",
    "RDD의 모든 요소에 특정한 함수를 적용하는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0978a215-e57f-472e-a278-abb9a3b2ed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "def f(df):\n",
    "    print(df.Seqno)\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "295db983-3a40-4abe-a532-37c8e9363b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "### foreach() with accumulator Example\n",
    "\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27366184-dbb9-4d20-bc74-7ab30df43d37",
   "metadata": {},
   "source": [
    "#### RDD.foreach(f: Callable[[T], None]) -> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efe838c9-6a83-4fcd-995f-fb603151d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# foreach() with RDD example\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value) #Accessed by driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf4139-58f2-469f-af3f-57642dd08558",
   "metadata": {},
   "source": [
    "### Random Sample\n",
    "pyspark.sql.DataFrame.sample()\\\n",
    "pyspark.sqlDataFrame.sampleBy()\\\n",
    "RDD.sample()\\\n",
    "RDD.takeSample()\n",
    "\n",
    "큰 사이즈의 데이터셋/파일로 처리하다보면 시간이 너무 많이 걸릴 수 있음, 분석할 동안은 큰 파일의 랜덤 부분집합 표본을 사용하는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e6cd6-c2c8-470a-9b97-7ab50c847b0a",
   "metadata": {},
   "source": [
    "Pyspark sampling (pyspark.sql.DataFrame.sample()) 은 데이터셋에서 랜덤 샘플 레코드를 얻는 과정임\\\n",
    "큰 데이터셋을 가지고 있을 때 원본 파일의 10% 정도같이 하위 집합을 분석/ 테스트 시 유용함\n",
    "### sample(withReplacement, fraction, seed = None)\n",
    "fraction - 생성할 행의 비율, 범위는 [0.0,1.0]임. 레코드 비율의 정확한 수를 보장하진 않음\\\n",
    "seed - 샘플링을 위한 seed\\\n",
    "withReplacement - 교체여부에 따른 샘플(default False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89238d01-ade8-4138-8cdf-d87ece56acca",
   "metadata": {},
   "source": [
    "### fraction 사용해서 random sample 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9bda5b-c983-48e8-875f-ebecd178ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=4), Row(id=23), Row(id=39), Row(id=99)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "df=spark.range(100)\n",
    "print(df.sample(0.06).collect())\n",
    "\n",
    "# 100레코드 중 6% 샘플 레코드를 요청했으나(6줄) , 7줄이 나옴 => 샘플 function은 지정된 정확한 비율을 반환하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ecf942-415d-42b0-9482-57d41c329754",
   "metadata": {},
   "source": [
    "### seed 사용해서 샘플 재사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4223a42-bf1e-49b9-9ed8-0081d6995a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=36), Row(id=37), Row(id=41), Row(id=43), Row(id=56), Row(id=66), Row(id=69), Row(id=75), Row(id=83)]\n",
      "[Row(id=36), Row(id=37), Row(id=41), Row(id=43), Row(id=56), Row(id=66), Row(id=69), Row(id=75), Row(id=83)]\n",
      "[Row(id=19), Row(id=21), Row(id=42), Row(id=48), Row(id=49), Row(id=50), Row(id=75), Row(id=80)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(0.1, 123).collect())\n",
    "print(df.sample(0.1, 123).collect())\n",
    "print(df.sample(0.1, 456).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6c620-153a-403d-9e1a-55e45f3a690f",
   "metadata": {},
   "source": [
    "### withReplacement\n",
    "반복되는 값의 랜덤샘플 얻고싶은경우, True value 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912735ea-3eb4-4655-a3f1-4bb6f39cb99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=0), Row(id=5), Row(id=9), Row(id=11), Row(id=14), Row(id=14), Row(id=16), Row(id=17), Row(id=21), Row(id=29), Row(id=33), Row(id=41), Row(id=42), Row(id=52), Row(id=52), Row(id=54), Row(id=58), Row(id=65), Row(id=65), Row(id=71), Row(id=76), Row(id=79), Row(id=85), Row(id=96)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(True, 0.3, 123).collect())\n",
    "#14,14,52,52,65,65...반복되는 값 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc01d5f1-f103-4a84-99b3-db1c7329d5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=0), Row(id=4), Row(id=17), Row(id=19), Row(id=24), Row(id=25), Row(id=26), Row(id=36), Row(id=37), Row(id=41), Row(id=43), Row(id=44), Row(id=53), Row(id=56), Row(id=66), Row(id=68), Row(id=69), Row(id=70), Row(id=71), Row(id=75), Row(id=76), Row(id=78), Row(id=83), Row(id=84), Row(id=88), Row(id=94), Row(id=96), Row(id=97), Row(id=98)]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(0.3, 123).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f2cc1-7e03-4649-a18a-502214c3bff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e950f184-f96b-466b-be3a-f8b4d076e2a8",
   "metadata": {},
   "source": [
    "### stratified sampling \n",
    "sampleBy()\\\n",
    "각 계층에 대한 샘플링 비율 리턴\n",
    "#### sampleBy(col, fractions, seed = None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61fe6d3-b052-4926-97d8-ca33d0b9f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(key=0), Row(key=1), Row(key=1), Row(key=1), Row(key=0), Row(key=1), Row(key=1), Row(key=0), Row(key=1), Row(key=1), Row(key=1)]\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select((df.id % 3).alias(\"key\"))\n",
    "print(df2.sampleBy(\"key\", {0: 0.1, 1:0.2}, 0).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85378d09-7111-4b7a-bcc5-e679bf277f7e",
   "metadata": {},
   "source": [
    "### RDD Sample\n",
    "RDD also provides sample(), takeSample() return an array[T]\n",
    "#### sample(self, withReplacement, fraction, seed = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1823a93f-8171-4f0f-bf08-4d4c8a735978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 29, 41, 64, 86]\n",
      "[0, 11, 13, 14, 16, 18, 21, 23, 27, 31, 32, 32, 48, 49, 49, 53, 54, 72, 74, 77, 77, 83, 88, 91, 93, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.range(0,100)\n",
    "print(rdd.sample(False, 0.1,0).collect())\n",
    "print(rdd.sample(True, 0.3, 123).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf26a0-e384-43c9-a58a-a45d0b5808f5",
   "metadata": {},
   "source": [
    "#### takeSample(self, withReplacement, num, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b4a7ea-b9e9-4a0d-a623-f45722b71312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 1, 96, 74, 29, 24, 32, 37, 94, 91]\n",
      "[43, 65, 39, 18, 84, 86, 25, 13, 40, 21, 79, 63, 7, 32, 26, 71, 23, 61, 83, 60, 22, 35, 84, 22, 0, 88, 16, 40, 65, 84]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.takeSample(False,10,0))\n",
    "print(rdd.takeSample(True, 30, 123))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad375f-2607-468d-b7f9-28ef1eb5eccc",
   "metadata": {},
   "source": [
    "### fillna() , fill() - replace NULL/None Values\n",
    "DataFrame.fillna() or DataFrameNaFunctions.fill()\\\n",
    "DataFrame 열의 NULL/None 값을 0, 빈 문자열, 공백, 상수로 바꾸는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c35d60-fcce-40f1-811b-5c72d8ab29bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|null               |PR   |30100     |\n",
      "|2  |704    |null    |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709    |null    |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|null               |TX   |null      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "\n",
    "filePath = \"small_zipcode.csv\"\n",
    "df = spark.read.options(header = 'true', inferSchema='true').csv(filePath)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af7de0-5ca8-4fc5-b539-843323b630a5",
   "metadata": {},
   "source": [
    "#### fillna(value, subset=None) fill(value, subset=None)\n",
    "DataFrame.fillna(), DataFrameNaFunctions.fill() replace NULL/None values\\\n",
    "value - int, long, float, string, dict...\\\n",
    "subset - optional, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42f331-3e20-4d0b-b2d3-562063f100dd",
   "metadata": {},
   "source": [
    "### replace NULL/None Values with Zero(0)\n",
    "fill(value:Long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d1ca07-bbdb-4e79-986c-0598afcdc417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Replace 0 for null for all integer columns\n",
    "\n",
    "df.na.fill(value=0).show()\n",
    "\n",
    "#Replace 0 for null on only population column \n",
    "\n",
    "df.na.fill(value=0, subset=[\"population\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d57648-913a-486b-8d2d-0c23890a9821",
   "metadata": {},
   "source": [
    "### replace Null/None with empty String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822e2961-a157-4f5f-8df1-4306d3cd5d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|                   |PR   |30100     |\n",
      "|2  |704    |        |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709    |        |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|                   |TX   |null      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a2a3c4-78f2-41dd-9fea-b2afb23a62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"unknown\", [\"city\"]).na.fill(\"\", [\"type\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ce4a44-bf85-4488-8b57-ad19ef75d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill({\"city\":\"unknown\", \"type\": \"\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370791e-3e1a-4f40-81d1-150da7fd2d89",
   "metadata": {},
   "source": [
    "### pivot() unpivot()\n",
    "Pivot() 그룹화된 열 값 중 하나를 고유한 데이터가 있는 개별 열로 바꾸는 집계입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9217b964-b05a-46dc-9f6d-17c157706a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "#Create spark session\n",
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5ff14c8-fcb6-4b25-a5b4-a03dce594afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f1ec1-1e4d-47dd-8d94-49a938c15240",
   "metadata": {},
   "source": [
    "버전 2.0 에서는 성능이 개선됐는데, 그 아래는 매우 비싼 작업임. 인수로 컬럼 데이터를 제공하는걸 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07aa4d5a-4fe7-4208-a8a4-b6a62b5d4829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+------+------+\n",
      "|Product|USA |China|Canada|Mexico|\n",
      "+-------+----+-----+------+------+\n",
      "|Orange |4000|4000 |null  |null  |\n",
      "|Beans  |1600|1500 |null  |2000  |\n",
      "|Banana |1000|400  |2000  |null  |\n",
      "|Carrots|1500|1200 |2000  |null  |\n",
      "+-------+----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = [\"USA\", \"China\", \"Canada\", \"Mexico\"]\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
    "pivotDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc2d61bb-8ad7-497d-a986-4b95599a8fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df.groupBy(\"Product\", \"Country\").sum(\"Amount\").groupBy(\"Product\")\\\n",
    ".pivot(\"Country\").sum(\"sum(Amount)\")\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17715b3-cc4b-4a90-a164-986c4e497081",
   "metadata": {},
   "source": [
    "### partitionBy()\n",
    "pyspark.sql.DataFrameWriter\\\n",
    "큰 데이터셋을 하나 이상의 컬럼을 기반으로 작은 파일로 분할하는데 사용\\\n",
    "데이터레이크에서 쿼리 성능을 개선하는 방법\\\n",
    "하나 이상의 파티션 키로 큰 데이터셋을 작은 데이터셋으로 나눔\\\n",
    "PySpark supports partition in two ways; partition in memory (DataFrame) and partition on the disk (File system).\n",
    "\n",
    "partition in memory: repartition() or coalesec()\\\n",
    "partition on disk: DataFrame 을 디스크에 다시 쓰는 동안, pyspark.sql.DataFrameWriter의 partitionBy() 를 사용해서 컬럼기반으로 데이터를 파티션 하는 방법을 선택할 수 있음 \n",
    "\n",
    "#### partitionBy(self, *cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f38b499a-80c4-432c-a588-b112496bf230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"simple-zipcodes.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f373a0f-2463-4087-8a6f-df12b96900e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\", True).partitionBy(\"state\").mode(\"overwrite\").csv(\"zipcodes-state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3190588-b77d-427f-ae7d-cf58ca3fe939",
   "metadata": {},
   "source": [
    "파티션으로 데이터를 쓰는 동안, 데이터파일에서 파티션 컬럼을 제거함\\\n",
    "저장공간이 절약됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb301a4-dda4-4000-9cd3-2b763b3864c6",
   "metadata": {},
   "source": [
    "### partitionBy() multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8974b8d4-940a-4527-b7b9-255f5a7a4162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|       49347|     US|               HOLT|  32564|   FL|\n",
      "|       49348|     US|          HOMOSASSA|  34487|   FL|\n",
      "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|\n",
      "|       61392|     US|         FORT WORTH|  76177|   TX|\n",
      "|       61393|     US|           FT WORTH|  76177|   TX|\n",
      "|       54356|     US|        SPRUCE PINE|  35585|   AL|\n",
      "|       76511|     US|           ASH HILL|  27007|   NC|\n",
      "|           4|     US|    URB EUGENE RICE|    704|   PR|\n",
      "|       39827|     US|               MESA|  85209|   AZ|\n",
      "|       39828|     US|               MESA|  85210|   AZ|\n",
      "|       49345|     US|           HILLIARD|  32046|   FL|\n",
      "|       49346|     US|             HOLDER|  34445|   FL|\n",
      "|           3|     US|      SECT LANAUSSE|    704|   PR|\n",
      "|       54354|     US|      SPRING GARDEN|  36275|   AL|\n",
      "|       54355|     US|        SPRINGVILLE|  35146|   AL|\n",
      "|       76512|     US|           ASHEBORO|  27203|   NC|\n",
      "|       76513|     US|           ASHEBORO|  27204|   NC|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.option(\"header\", True).partitionBy(\"state\", \"city\").mode(\"overwrite\").csv(\"zipcodes-state\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c900b80-cab1-40e7-9c3d-9d54e5a06e6f",
   "metadata": {},
   "source": [
    "### using repartition() and partitionBy() together\n",
    "repartition()은 지정된 수의 파티션을 메모리에 만듦\\\n",
    "partitionBy()는 각 메모리 파티션과 파티션 컬럼에 대해 파일을 디스크에 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75407dc5-d266-4b96-943d-fc052fcaf968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(2).write.option(\"header\", True).partitionBy(\"state\").mode(\"overwrite\").csv(\"zipcodes-state-more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef5ab5-7f61-4a1a-93fe-8462636aadec",
   "metadata": {},
   "source": [
    "### Data Skew\n",
    "파티션 파일당 레코드 수 제어 \\\n",
    "maxRecordsPerFile \\\n",
    "데이터가 왜곡됨(일부 파티션에는 레코드 수가 매우 적고 다른 파티션에는 레코드 수가 많음)현상에서 특히 도움됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b71556-2162-4b8d-a8be-f89a33e6bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\", True).option(\"maxRecordsPerFile\",2).partitionBy(\"state\")\\\n",
    ".mode(\"overwrite\").csv(\"zipcodes-state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5dac6-865a-4703-b464-ad28ec56c233",
   "metadata": {},
   "source": [
    "### read a specific partition\n",
    "전체 파일 스캔하는 대신 특정 폴더에서 데이터 읽음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5233d670-ec47-4326-9315-1cda0723e249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      "\n",
      "+------------+-------+-------+\n",
      "|RecordNumber|Country|Zipcode|\n",
      "+------------+-------+-------+\n",
      "|       54355|     US|  35146|\n",
      "+------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSinglePart = spark.read.option(\"header\", True).csv(\"zipcodes-state/state=AL/city=SPRINGVILLE\")\n",
    "dfSinglePart.printSchema()\n",
    "dfSinglePart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58dccaa-29ec-4cea-a6f8-6f4c41ddd731",
   "metadata": {},
   "source": [
    "특정 파티션 데이터를 읽었을 때, 파티션 컬럼을 포함하지 않는 걸 볼 수 있음\n",
    "\n",
    "\n",
    "데이터 셋 전체 폴더로 읽으면 또 스키마랑 데이터에 포함돼서 읽혀짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59740c2-ca19-4221-beab-8f32c9245a4e",
   "metadata": {},
   "source": [
    "### SQL - Read Partition Data\n",
    "파티션이 없는 쿼리보다 훨씬 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17ff049d-9804-4869-931c-624a30ff0f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+-----+-----------+\n",
      "|RecordNumber|Country|Zipcode|state|       city|\n",
      "+------------+-------+-------+-----+-----------+\n",
      "|       54355|     US|  35146|   AL|SPRINGVILLE|\n",
      "+------------+-------+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parqDF = spark.read.option(\"header\", True).csv(\"zipcodes-state\")\n",
    "parqDF.createOrReplaceTempView(\"ZIPCODE\")\n",
    "spark.sql(\"select * from ZIPCODE where state = 'AL' and city = 'SPRINGVILLE'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d1e9d-546c-4c5e-b810-1a669c56870b",
   "metadata": {},
   "source": [
    "### How to Choose a Partition Column When Writing to File System?\n",
    "파티션 개수 정할 때 조심해야함, 너무 많은 파티션을 많은 서브디렉터리를 생성해서 오버헤드 줄 수 있음(하둡쓴다면)\\\n",
    "since it must keep all metadata for the file system in memory.\\\n",
    ". Ideally, you should partition on Year/Month but not on a date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fae89-b5ad-40cc-b23a-6037ccd8afcc",
   "metadata": {},
   "source": [
    "### MapType(Dict)\n",
    "key-value 쌍으로 저장하는 Python Dictionary 타입 나타냄\\\n",
    "key type, value type, valueContainsNull(BooleanType) 세가지 필드로 구성\\\n",
    "pyspark.sql.types.MapType 사용하여 MapType() 객체 생성\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753e4a07-67b3-4fe8-aba5-752c4df36415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, MapType\n",
    "mapCol = MapType(StringType(), StringType(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2bdc9-3031-490e-9d57-692130dc310c",
   "metadata": {},
   "source": [
    "1 매개변수 KeyType 맵의 키 유형 지정 시 사용\\\n",
    "2 매개변수 ValueType 맵의 값 유형 지정 시 사용\\\n",
    "3 매개변수 ValueContainsNull \b옵션 boolean type, 두번째 매개변수 값이 null/none 값을 허용하는지 여부를 지정하는 데 사용\\\n",
    "map의 key는 None/Null 값을 허용X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bc1fa-7864-460f-b8e7-de4ade8cbbdb",
   "metadata": {},
   "source": [
    "### MapType From StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f834f31e-dc5a-4062-99b5-e194ddd50b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', MapType(StringType(), StringType()), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271566fb-2c15-4311-a097-0011dfc74509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-----------------------------+\n",
      "|name      |properties                   |\n",
      "+----------+-----------------------------+\n",
      "|James     |{eye -> brown, hair -> black}|\n",
      "|Michael   |{eye -> null, hair -> brown} |\n",
      "|Robert    |{eye -> black, hair -> red}  |\n",
      "|Washington|{eye -> grey, hair -> grey}  |\n",
      "|Jefferson |{eye -> , hair -> brown}     |\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown','eye':''})\n",
    "        ]\n",
    "df = spark.createDataFrame(data = dataDictionary, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef523fa7-120d-44b5-8a57-4172cca04a22",
   "metadata": {},
   "source": [
    "### MapType 요소 접근\n",
    "Dictionary 컬럼에서 key, value 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243b54ab-a52f-4323-b932-45a00e3f6dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- hair: string (nullable = true)\n",
      " |-- eye: string (nullable = true)\n",
      "\n",
      "+----------+-----+-----+\n",
      "|      name| hair|  eye|\n",
      "+----------+-----+-----+\n",
      "|     James|black|brown|\n",
      "|   Michael|brown| null|\n",
      "|    Robert|  red|black|\n",
      "|Washington| grey| grey|\n",
      "| Jefferson|brown|     |\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.rdd.map(lambda x:(x.name, x.properties[\"hair\"], x.properties[\"eye\"])).toDF([\"name\",\"hair\",\"eye\"])\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e080861e-bf7e-4f1e-a785-a35f091b696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|name      |hair |eye  |\n",
      "+----------+-----+-----+\n",
      "|James     |black|brown|\n",
      "|Michael   |brown|null |\n",
      "|Robert    |red  |black|\n",
      "|Washington|grey |grey |\n",
      "|Jefferson |brown|     |\n",
      "+----------+-----+-----+\n",
      "\n",
      "+----------+-----+-----+\n",
      "|      name| hair|  eye|\n",
      "+----------+-----+-----+\n",
      "|     James|black|brown|\n",
      "|   Michael|brown| null|\n",
      "|    Robert|  red|black|\n",
      "|Washington| grey| grey|\n",
      "| Jefferson|brown|     |\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"hair\", df.properties.getItem(\"hair\"))\\\n",
    ".withColumn(\"eye\", df.properties.getItem(\"eye\"))\\\n",
    ".drop(\"properties\")\\\n",
    ".show(truncate = False)\n",
    "\n",
    "\n",
    "df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n",
    "  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n",
    "  .drop(\"properties\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4fce59-dd2f-4952-a540-54af04495094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| null|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|black|\n",
      "|    Robert|hair|  red|\n",
      "|Washington| eye| grey|\n",
      "|Washington|hair| grey|\n",
      "| Jefferson| eye|     |\n",
      "| Jefferson|hair|brown|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name, explode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f92b230-0c9c-4945-90d9-f5cde3b03d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      name|map_keys(properties)|\n",
      "+----------+--------------------+\n",
      "|     James|         [eye, hair]|\n",
      "|   Michael|         [eye, hair]|\n",
      "|    Robert|         [eye, hair]|\n",
      "|Washington|         [eye, hair]|\n",
      "| Jefferson|         [eye, hair]|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "df.select(df.name, map_keys(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e33fb593-7ca1-43e9-b1dd-4b05b597a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eye', 'hair']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,map_keys\n",
    "keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
    "keyList = keysDF.rdd.map(lambda x:x[0]).collect()\n",
    "print(keyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f71a64a-1d7a-40ef-bb29-430c8c0851bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      name|map_values(properties)|\n",
      "+----------+----------------------+\n",
      "|     James|        [brown, black]|\n",
      "|   Michael|         [null, brown]|\n",
      "|    Robert|          [black, red]|\n",
      "|Washington|          [grey, grey]|\n",
      "| Jefferson|             [, brown]|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "df.select(df.name, map_values(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4230d5-6df9-44b7-941c-7fd57f5e35c7",
   "metadata": {},
   "source": [
    "### Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2de227cb-6ffd-4ea4-a8f0-4a2fbe2eec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c5ef8df-de01-4819-a087-8ff23a2d5a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distint:6\n"
     ]
    }
   ],
   "source": [
    "#approx_count_distinct()\n",
    "# 그룹에 distinct item 수를 리턴\n",
    "# salary에 고유한 값들 개수\n",
    "print(\"approx_count_distint:\" + str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c26d8b20-ab02-4406-afec-73cfb83fc8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 3400.0\n"
     ]
    }
   ],
   "source": [
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "545663b4-4d08-485e-bdaf-9bcbcf331ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the collect_list function has grouped the data by name and created a list of all the course values for each name.\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "072da523-842a-41c3-aa58-cd14df42943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect_set => distinct value 리턴함\n",
    "df.select(collect_set(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eacf144d-fb26-44b8-966c-61ba249ec51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|8                                 |\n",
      "+----------------------------------+\n",
      "\n",
      "Distinct Count of Department & Salary: 8\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate = False)\n",
    "print(\"Distinct Count of Department & Salary: \" +str(df2.collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed143125-4f37-4ae5-a76d-c31e8e3e6c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:Row(count(salary)=10)\n"
     ]
    }
   ],
   "source": [
    "print(\"count:\"+ str(df.select(count(\"salary\")).collect()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2f219cb-fe2c-4fc6-a9c5-a0cb2f3d010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|3000         |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(first(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b1d1750-62b6-428e-8c5f-23967329bc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(last(\"salary\")).show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be3caec1-8a54-48b9-909e-036d82f7fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6467803030303032|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(kurtosis(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f442d3dd-d550-4661-af98-0b89b88656b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b2efccc-d8f8-4168-9b18-79c9dad72f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a135993c-4c4d-470f-9b82-caff6fe63429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|3400.0     |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(mean(\"salary\")).show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5672e9f-cda8-4f05-bfdf-a1a910825e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.12041791181069571|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(skewness(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c81e7aaf-ea2b-40d8-861e-7e2c217dc78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|765.9416862050705  |765.9416862050705  |726.636084983398  |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), stddev_pop(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "989808fa-1179-4e44-af46-970e0c223c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|34000      |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sum(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4861d3d4-e9f6-4295-9512-9d25a0f75e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/functions.py:752: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|20900               |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sumDistinct(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8645dd83-3cea-4461-84a5-92205581c1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+---------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
      "+-----------------+-----------------+---------------+\n",
      "|586666.6666666666|586666.6666666666|528000.0       |\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(variance(\"salary\"), var_samp(\"salary\"), var_pop(\"salary\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b133f-53a5-430e-9a5a-10b05e7c6740",
   "metadata": {},
   "source": [
    "### Window Functions\n",
    "입력 행 범위에 대한 순위, 행 번호 등과 같은 결과 계산하는데 쓰임\\\n",
    "로우 그룹에서 작동하며 모든 입력 로우에 대해 단일 값을 리턴함\\\n",
    "그룹에 대한 작업 수행하려면 Window.partitionBy()로 데이터를 먼저 파티션해야하며,\\\n",
    "행 번호, 순위 함수에 대해 추가로 파티션 데이터를 orderBy 로 정렬해야함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a813e795-0564-40b6-a845-6805d7f1dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    "\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4282e-dfef-4078-8dac-cc695fec6640",
   "metadata": {},
   "source": [
    "WindowSpec창 ( 프레임 ) 에 어떤 행이 포함되는지를 정의하는 창 사양 입니다 . 즉, 일부 관계 에 의해 현재 행과 연관된 행 집합입니다 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f7907-9c7a-46d9-be3b-5e093e7116f5",
   "metadata": {},
   "source": [
    "### row_number()\n",
    "각 윈도우 파티션 행의 순서 번호 1부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cfceed9-876a-4f18-a330-e66932456c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_numver|\n",
      "+-------------+----------+------+----------+\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_numver\", row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ee289-9015-4d8b-8484-fb497ee30387",
   "metadata": {},
   "source": [
    "### rank()\n",
    "윈도우 파티션 내 순위\\\n",
    "동점일 때 순위 차이를 남김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44eaa8e1-b4db-47bc-b52c-234f3b9e4cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)).show()\n",
    "\n",
    "# 동점일 때 건너뛴 거 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186c364-655f-4adf-a435-a6325477e1c0",
   "metadata": {},
   "source": [
    "### dense_rank()\n",
    "차이 없이 순위 매기는 함수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5cc5e03-de13-4839-b158-c471b4f225be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb4245-de3d-46d7-8f9b-7492f6ee6d0d",
   "metadata": {},
   "source": [
    "### percent_rank()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de303bd4-8cd6-4f64-8e5d-384258b915de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\", percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a7aad-4203-4265-a43a-bf5909e56974",
   "metadata": {},
   "source": [
    "### ntile()\n",
    "상대적인 순위\\\n",
    "2 argument, 1&2 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af38f098-01fb-4b67-a4af-49309686bc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\", ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19e398-6ec4-4339-a358-8c6cf88b1d1e",
   "metadata": {},
   "source": [
    "### Window Analytic functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8e484-cfb5-4a87-a5d9-33f9b6afecb2",
   "metadata": {},
   "source": [
    "### cume_dist()\n",
    "값의 누적 cumulative 분포 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04209e3c-5e1c-4ad8-89ad-7469629c570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\", cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea9c89-45f2-445c-9ba0-f7f91f77d8d2",
   "metadata": {},
   "source": [
    "### lag()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73c7914-11fa-40e2-be65-980c2018488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "df.withColumn(\"lag\" , lag(\"salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a742a-3904-44ff-b091-cf78e8962116",
   "metadata": {},
   "source": [
    "### lead()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42204d5c-6b9d-4b39-b8f8-98b502b00d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "df.withColumn(\"lead\", lead(\"salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78fc48-5f26-4cb1-a3a3-1eaa3fb3ad24",
   "metadata": {},
   "source": [
    "### window aggregate functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1189cdd0-324b-440c-98e0-3b4ee29ac785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    "\n",
    "df.withColumn(\"row\", row_number().over(windowSpec)).\\\n",
    "withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg))\\\n",
    ".withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg))\\\n",
    ".withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg))\\\n",
    ".withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg))\\\n",
    ".where(col(\"row\") == 1).select(\"department\", \"avg\", \"sum\", \"min\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c5680-aefc-4046-9993-4610a4d6cf91",
   "metadata": {},
   "source": [
    "### Date and Timestamp \n",
    "DateType default format yyyy-MM-dd\\\n",
    "TimestampType default yyyy-MM-dd HH:mm:ss.SSSS\\\n",
    "retrun null, 입력이 date/ Timestamp 로 cast 되지 않는  string 일 경우\\\n",
    "https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028cfbdc-7cdb-4553-906d-2a5ad99cbbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed479fe8-7f45-4369-9ee2-b02350306c61",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/\n",
    "### current_date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5ebd3a-e2cd-4463-9cb5-eceba1fc7b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2023-12-05|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias(\"current_date\")).show(1)\n",
    "\n",
    "#By default, the data will be returned in yyyy-dd-mm format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3bd4d-6b23-420f-85a9-2014e2620125",
   "metadata": {},
   "source": [
    "### date_format()\n",
    "parses the date and converts from yyyy-dd-mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c38fcac-84ef-4de5-b81e-6e4cb5ea6c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|     input|date_format|\n",
      "+----------+-----------+\n",
      "|2020-02-01| 02-01-2020|\n",
      "|2019-03-01| 03-01-2019|\n",
      "|2021-03-01| 03-01-2021|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"),date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4586d9e-4cf2-4470-859c-9d312e39dbaa",
   "metadata": {},
   "source": [
    "### to_date()\n",
    "문자열을 날짜 형식으로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350079cf-4bac-4e6f-809b-597843cab0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     input|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), to_date(col(\"input\"),\"yyyy-MM-dd\").alias(\"to_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973e279-9f53-4172-acd2-983b18c3bdcc",
   "metadata": {},
   "source": [
    "### datediff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc5a463-57c2-48e8-b5ec-abeef88f2150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|     input|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|    1403|\n",
      "|2019-03-01|    1740|\n",
      "|2021-03-01|    1009|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), datediff(current_date(), col(\"input\")).alias(\"datediff\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe89a617-ca29-4812-826f-3272f37eef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|     input|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|       1|\n",
      "|2019-03-01|     338|\n",
      "|2021-03-01|    -393|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), datediff(lit('2020-02-02'), col(\"input\")).alias(\"datediff\")).show()\n",
    "#lit 으로 열에 상수값 채운다음에 뺴기 -> 아마 오른쪽에 빼는 값이 열의 값이라서 수가 맞는 열 요소가 필요한가봄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797a354-dc34-4a88-b8a2-64f0264522b9",
   "metadata": {},
   "source": [
    "### months_between()\n",
    "return 두 날짜 사이의 개월 수 계산하기\\\n",
    "MONTHS_BETWEEN(날짜, 날짜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18f6d90c-9661-47a0-abcf-783cf54b6026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|     input|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   46.09677419|\n",
      "|2019-03-01|   57.09677419|\n",
      "|2021-03-01|   33.09677419|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), months_between(current_date(), col(\"input\")).alias(\"months_between\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f71288-7615-4de4-9a33-bac4ee907763",
   "metadata": {},
   "source": [
    "### trunc()\n",
    "truncates the date at a specified unit\\\n",
    "TRUNC(DT, 'YEAR')\t-- 월, 일 초기화\\\n",
    "     , TRUNC(DT, 'MONTH')\t-- 일 초기화\\\n",
    "     , TRUNC(DT, 'DAY')\t\t-- 요일 초기화 (일요일)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5531674-65fb-4ac1-a143-80e90b9af288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+\n",
      "|     input|Month_Trunc|Month_Year|Month_Trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n",
      "|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n",
      "|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), \\\n",
    "          trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"),\n",
    "          trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"),\n",
    "          trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714d345-7307-45e2-b01c-c20f0e22109d",
   "metadata": {},
   "source": [
    "### add_months(), date_add(), date_sub()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72fdfe06-6c0c-481f-8610-5269a9866e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), \\\n",
    "          add_months(col(\"input\"),3).alias(\"add_months\"),\n",
    "          add_months(col(\"input\"),-3).alias(\"sub_months\"),\n",
    "          date_add(col(\"input\"),4).alias(\"date_add\"),\n",
    "          date_sub(col(\"input\"),4).alias(\"date_sub\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ed78f-990e-435a-af64-ddbd7307a5a2",
   "metadata": {},
   "source": [
    "### year(), month(), month(),next_day(), weekofyear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "914f72b2-6fdb-435a-864b-f53d80d85143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5202d91-0edf-4c89-80fe-cd07a99e9d17",
   "metadata": {},
   "source": [
    "### dayofweek(), dayofmonth(), dayofyear()\n",
    "#dayofweek 1 for sunday, 2 for monday, ...7 for saturday\\\n",
    "#dayofmonth 며칠인지 반환\\\n",
    "#dayofyear 년을 기준으로 며칠째인지 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afcebe9b-523d-4c88-b6cc-978c5543f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|     input|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
    "  ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4ef22-f784-409f-9c4a-b5732230fe34",
   "metadata": {},
   "source": [
    "### current_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdace92e-905c-4142-a832-deb768ad2e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d0209d-5ab4-48f4-819b-c3a9dde64e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|current_timestamp         |\n",
      "+--------------------------+\n",
      "|2023-12-05 01:57:52.098615|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(current_timestamp().alias(\"current_timestamp\")).show(1,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b03bd-1a28-4943-a853-29524f5b3eea",
   "metadata": {},
   "source": [
    "### to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "378260b7-70bc-4b20-b12e-0ec012461137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(col(\"input\"), to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07f224-25b7-48c9-bf32-20a4234dc573",
   "metadata": {},
   "source": [
    "### hour(), Minute() and second()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebf9380e-86b2-4e50-8298-7e7a19816029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc89644-3230-48c7-bc6a-34c370b6621e",
   "metadata": {},
   "source": [
    "### JSON functions\n",
    "#### from_json() \n",
    "json string 을 struct type, map type 으로 바꿈\n",
    "#### to_json() \n",
    "map type, struct type 을 json string 으로 바꿈\n",
    "#### json_tuple()\n",
    "json에서 데이터 추출해서 새로운 컬럼으로 만듦\n",
    "#### get_json_object()\n",
    "지정된 json 경로를 기반으로 JSON 문자열에서 JSON 요소를 추출\n",
    "#### schema_of_json()\n",
    "json string으로부터 스키마 string 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bd9c148-a752-4c9a-ab20-a1009f0908bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame with JSON String\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df = spark.createDataFrame([(1, jsonString)], [\"id\", \"value\"])\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb51163b-0218-4edb-9d68-ebe76cffee1c",
   "metadata": {},
   "source": [
    "### from_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbd6447-0319-4128-a085-6fc880ab74c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|id |value                                                                      |\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n",
      "+---+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType, StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "#Convert JSON string column to Map type\n",
    "df2 = df.withColumn(\"value\", from_json(df.value, MapType(StringType(),StringType())))\n",
    "df2.printSchema()\n",
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21da3b0-cd10-43cd-9aab-e18eef3cea6b",
   "metadata": {},
   "source": [
    "### to_json()\n",
    "convert DataFrame columns MapType or Struct type to JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf7ce9f-5797-45b1-a315-a9a33da9adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------+\n",
      "|id |value                                                                       |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json, col\n",
    "df2.withColumn(\"value\", to_json(col(\"value\"))).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a623ee9-a66c-4e50-a5ec-2bc106f19201",
   "metadata": {},
   "source": [
    "### json_tuple()\n",
    " query or extract the elements from JSON column and create the result as a new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3bd5ea-66b5-45ba-ae58-0b638a33e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+-----------+\n",
      "|id |Zipcode|ZipCodeType|City       |\n",
      "+---+-------+-----------+-----------+\n",
      "|1  |704    |STANDARD   |PARC PARQUE|\n",
      "+---+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "df.select(col(\"id\"), json_tuple(col(\"value\"), \"Zipcode\", \"ZipCodeType\", \"City\")).toDF(\"id\", \"Zipcode\", \"ZipCodeType\", \"City\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474144ad-b72b-4721-ad97-30247a2ac1aa",
   "metadata": {},
   "source": [
    "### get_json_object()\n",
    "json column path 기반으로 json string 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7c0aab-3e89-49f0-b60c-0e8d200c0809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id |ZipCodeType|\n",
      "+---+-----------+\n",
      "|1  |STANDARD   |\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "df.select(col(\"id\"), get_json_object(col(\"value\"), \"$.ZipCodeType\").alias(\"ZipCodeType\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93bb04-beeb-4401-871c-658957346247",
   "metadata": {},
   "source": [
    "### schema_of_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b6e0b4-a21d-4bf0-becb-9d949cc5bbce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m schema_of_json, lit\n\u001b[0;32m----> 2\u001b[0m schemaStr \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m1\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(schema_of_json(lit(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZipcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:704,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZipCodeType\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTANDARD\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPARC PARQUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPR\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m))) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(schemaStr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import schema_of_json, lit\n",
    "schemaStr = spark.range(1)\\\n",
    "    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
    "    .collect()[0][0]\n",
    "print(schemaStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e3dac-2768-4232-a4aa-94bbb9074bba",
   "metadata": {},
   "source": [
    "### Read CSV File into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b936ab-f364-458d-bae6-a479dd4b8077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "df = spark.read.csv(\"zipcodes.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee368724-7df6-4b73-84b2-6da6051f1239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").load(\"zipcodes.csv\")\n",
    "#or\n",
    "#df = spark.read.format(\"org.apache.spark.sql.csv\").load(\"zipcodes.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9d5b3-5561-4c67-91da-0f7f4baf8323",
   "metadata": {},
   "source": [
    "### header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc09f75-f538-4960-b5f4-a542b7238a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Xaxis: string (nullable = true)\n",
      " |-- Yaxis: string (nullable = true)\n",
      " |-- Zaxis: string (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: string (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: string (nullable = true)\n",
      " |-- TotalWages: string (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"header\", True).csv(\"zipcodes.csv\")\n",
    "df2.printSchema()\n",
    "#컬럼 이름으로 헤더를 갖고 있다면, header option에 True 지정해라-> option(\"header\", True), 이거 안하면 그냥 데이터레코드로 취급함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f0da94-649d-4f45-b950-dec44b7cc2c9",
   "metadata": {},
   "source": [
    "### read multiple CSV \n",
    "df = spark.read.csv(\"path1, path2, path3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37489ef2-8ab0-499e-a00b-252feadefbd7",
   "metadata": {},
   "source": [
    "### read all CSV Files in a Directory\n",
    "df = spark.read.csv(\"Folder path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249b2d9-ba68-405c-81ce-b54d8138fddf",
   "metadata": {},
   "source": [
    "### delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc11ee33-e695-4edf-b3a0-775e0cbe2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.options(delimiter =',').csv(\"zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb5af7-a915-4b29-bf73-2a03c1065524",
   "metadata": {},
   "source": [
    "### inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ff0a6e0-aa40-4578-ad73-47433c84b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#True 하면 자동으로 컬럼타입 추론함\n",
    "df4 = spark.read.options(inferSchema = 'True', delimiter =',').csv(\"zipcodes.csv\")\n",
    "df4.printSchema()\n",
    "\n",
    "#option 함수로 바꿔도됨\n",
    "df4 = spark.read.option(\"inferSchema\", True).option(\"delimiter\", \",\").csv(\"zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e192b3-8da1-4dcb-aef7-af08bf0569f0",
   "metadata": {},
   "source": [
    "### header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "438d3e98-737b-4960-a190-339f9166c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: integer (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.options(header = 'True', inferSchema = 'True', delimiter = ',').csv(\"zipcodes.csv\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d86cb-9b77-46cf-89b6-1edc21219b01",
   "metadata": {},
   "source": [
    "### write dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f333a11-b8ad-4b53-a961-d46951a36661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\", True).csv(\"zipcodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9388bc5e-2388-4e02-81bb-25ff3fceb7a9",
   "metadata": {},
   "source": [
    "### options \n",
    "Other options available quote,escape,nullValue,dateFormat,quoteMode .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb98a20-5de0-4288-9f1d-f3bd543b3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    " df2.write.options(header = 'True', delimiter=',').csv(\"zipcodes2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8b73c-fabc-4401-b2e1-f8b81e54f92f",
   "metadata": {},
   "source": [
    "### saving modes\n",
    "overwrite\\\n",
    "append\\\n",
    "ignore\\\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00a995d-2acf-4a6f-baee-1f0ec6e725ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode('overwrite').csv(\"zipcodes\")\n",
    "#==\n",
    "df2.write.format(\"csv\").mode('overwrite').save(\"zipcodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731842a-37ce-40ce-8e93-a108aa961858",
   "metadata": {},
   "source": [
    "### Read and write Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e642e703-1fe8-447d-a9c1-0e20d47cc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"parquetFile\").getOrCreate()\n",
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfde9287-611b-4739-b6e7-711484d9ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"people.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040231ec-9a5c-44a0-b87c-51050f309de9",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840c1c55-296b-44aa-9bf0-aadd8886864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Read parquet file into DataFrame\n",
    "parDF = spark.read.parquet(\"people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb195fe-e991-44d2-90d9-bd37d15e3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59146d-cfd3-4318-8a69-dc1586c43b7d",
   "metadata": {},
   "source": [
    "### append or Overwrite an existing Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d38f3e48-aade-4223-ace3-402e9e88cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"append\").parquet(\"people.parquet\")\n",
    "df.write.mode(\"overwrite\").parquet(\"people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef27d58-59ee-4a91-a009-a69c90f2ce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8bcbf2-01c4-4f8e-b98d-36612acf4591",
   "metadata": {},
   "source": [
    "### creating a table on parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04466c0-7682-47a9-a836-eb45d371c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create temporary view person1 using parquet options (path \\\"people.parquet\\\")\")\n",
    "spark.sql(\"select * from person1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12796aa7-b6cd-4ec2-84da-c6b15eec0348",
   "metadata": {},
   "source": [
    "### create parquet partition file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd8ffa70-97f5-41ff-a839-7db4f2cac998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"gender\", \"salary\").mode(\"overwrite\").parquet(\"people2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb883b-3cef-4d69-ad19-42d9c54059b3",
   "metadata": {},
   "source": [
    "### retrieving from a partitioned parqeut file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31fcb580-1b14-4b49-8714-a1e6cb586980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|dob  |salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|Robert   |          |Williams|42114|4000  |\n",
      "|Michael  |Rose      |        |40288|4000  |\n",
      "|James    |          |Smith   |36636|3000  |\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parDF2 = spark.read.parquet(\"people2.parquet/gender=M\")\n",
    "parDF2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e2d7c-5c5e-49af-ab8a-c9952e9466fb",
   "metadata": {},
   "source": [
    "### creating a table on Partitioned Parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6f6b47b-3090-4b3c-9b8a-8c681934c5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|  dob|salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|   Maria |      Anne|   Jones|39192|  4000|\n",
      "|      Jen|      Mary|   Brown|     |    -1|\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create temporary view person2 using parquet options (path \\\"people2.parquet/gender=F\\\")\")\n",
    "spark.sql(\"select * from person2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac9e6d-d380-4284-a9e3-16a6e674c052",
   "metadata": {},
   "source": [
    "### lit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e60597fa-9c8f-4ff2-a618-94a9ab29ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
    "columns= [\"EmpId\",\"Salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ead640e-2754-499a-a366-0aadabbee868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|EmpId|Salary|lit_value1|\n",
      "+-----+------+----------+\n",
      "|111  |50000 |1         |\n",
      "|222  |60000 |1         |\n",
      "|333  |40000 |1         |\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "df2 = df.select(col(\"EmpId\"), col(\"Salary\"), lit(\"1\").alias(\"lit_value1\"))\n",
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6cdf0d-3152-4aaf-85e9-1d8e06b351fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
